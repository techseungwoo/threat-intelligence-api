import json
import csv
import os
import glob
import re
import pandas as pd
import logging
import sqlite3
import hashlib
import uuid
from difflib import SequenceMatcher
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from collections import defaultdict

class ThreatProcessingSystem:
    
    def __init__(self, db_path: str = 'threat_intelligence.db'):
        self.db_path = db_path
        self.setup_logging()
        
    def setup_logging(self):
        #ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('threat_processing.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def get_local_time(self):
        
        kst = timezone(timedelta(hours=9))
        return datetime.now(kst).strftime('%Y-%m-%d %H:%M:%S')

    # ==========================================================================
    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° ìŠ¤í‚¤ë§ˆ ì„¤ê³„
    # ==========================================================================
    
    def init_advanced_database(self):
        #ì—°ê´€ê´€ê³„ë¥¼ ì§€ì›í•˜ëŠ” ê³ ê¸‰ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # 1. ë©”ì¸ ìœ„í˜‘ì •ë³´ ê²Œì‹œë¬¼ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS threat_posts (
                id TEXT PRIMARY KEY,              -- ê³ ìœ  ê²Œì‹œë¬¼ ID
                source_type TEXT NOT NULL,        -- ì†ŒìŠ¤ íƒ€ì… (telegram/darkweb)
                thread_id TEXT,                   -- ì›ë³¸ ìŠ¤ë ˆë“œ/ë©”ì‹œì§€ ID
                url TEXT,                         -- ê²Œì‹œë¬¼ URL
                keyword TEXT,                     -- ê²€ìƒ‰ í‚¤ì›Œë“œ
                found_at TIMESTAMP,               -- ë°œê²¬ ì‹œê°„
                title TEXT,                       -- ê²Œì‹œë¬¼ ì œëª©
                text TEXT,                        -- ê²Œì‹œë¬¼ ë‚´ìš©
                author TEXT,                      -- ì‘ì„±ì/ì±„ë„ëª…
                date TIMESTAMP,                   -- ì‘ì„±ì¼
                threat_type TEXT,                 -- ìœ„í˜‘ ìœ í˜•
                platform TEXT,                    -- í”Œë«í¼ ì •ë³´
                data_hash TEXT UNIQUE,            -- ì¤‘ë³µ ê²€ì‚¬ìš© í•´ì‹œ
                created_at TIMESTAMP DEFAULT (datetime('now', 'localtime')),  -- DB ì €ì¥ ì‹œê°„
                event_id TEXT,                  -- ì´ë²¤íŠ¸ ID 
                event_info TEXT,                -- ì´ë²¤íŠ¸ ê´€ë ¨ ì •ë³´                
                event_date TIMESTAMP            -- ì´ë²¤íŠ¸ ë°œìƒì¼     
            )
        ''')
        
        # 2. IOC(ìœ„í˜‘ì§€í‘œ) ì •ë³´ í…Œì´ë¸” - ê° IOCë¥¼ ë³„ë„ë¡œ ê´€ë¦¬
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS threat_iocs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                post_id TEXT NOT NULL,            -- ì—°ê²°ëœ ê²Œì‹œë¬¼ ID
                ioc_type TEXT NOT NULL,           -- IOC íƒ€ì… (email, ip, hash ë“±)
                ioc_value TEXT NOT NULL,          -- IOC ì‹¤ì œ ê°’
                context TEXT,                     -- IOCê°€ ë°œê²¬ëœ ë§¥ë½
                confidence REAL DEFAULT 1.0,     -- ì‹ ë¢°ë„ (0.0 ~ 1.0)
                first_seen TIMESTAMP DEFAULT (datetime('now', 'localtime')),
                FOREIGN KEY (post_id) REFERENCES threat_posts(id) ON DELETE CASCADE
            )
        ''')
        
        # 3. ê²Œì‹œë¬¼ ê°„ ì—°ê´€ê´€ê³„ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS post_relationships (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                post_id_1 TEXT NOT NULL,          -- ì²« ë²ˆì§¸ ê²Œì‹œë¬¼ ID
                post_id_2 TEXT NOT NULL,          -- ë‘ ë²ˆì§¸ ê²Œì‹œë¬¼ ID
                relationship_type TEXT NOT NULL,   -- ê´€ê³„ íƒ€ì…
                similarity_score REAL,            -- ìœ ì‚¬ë„ ì ìˆ˜ (0.0 ~ 1.0)
                description TEXT,                 -- ê´€ê³„ ì„¤ëª…
                created_at TIMESTAMP DEFAULT (datetime('now', 'localtime')),
                FOREIGN KEY (post_id_1) REFERENCES threat_posts(id) ON DELETE CASCADE,
                FOREIGN KEY (post_id_2) REFERENCES threat_posts(id) ON DELETE CASCADE,
                UNIQUE(post_id_1, post_id_2)     -- ì¤‘ë³µ ê´€ê³„ ë°©ì§€
            )
        ''')
        
        # 4. ì²˜ë¦¬ í†µê³„ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS processing_statistics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                batch_id TEXT,                    -- ë°°ì¹˜ ì²˜ë¦¬ ID
                source_files TEXT,                -- ì²˜ë¦¬ëœ ì†ŒìŠ¤ íŒŒì¼ë“¤
                total_input INTEGER,              -- ì…ë ¥ ë°ì´í„° ìˆ˜
                new_posts INTEGER,                -- ìƒˆë¡œ ìƒì„±ëœ ê²Œì‹œë¬¼
                related_posts INTEGER,            -- ì—°ê´€ê´€ê³„ë¡œ ì²˜ë¦¬ëœ ê²Œì‹œë¬¼
                duplicate_posts INTEGER,          -- ì™„ì „ ì¤‘ë³µ ê²Œì‹œë¬¼
                error_count INTEGER,              -- ì˜¤ë¥˜ ë°œìƒ ìˆ˜
                processing_time_seconds REAL,    -- ì²˜ë¦¬ ì‹œê°„
                processed_at TIMESTAMP DEFAULT (datetime('now', 'localtime'))
            )
        ''')
        
        # 5. ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
        indexes = [
            # ê¸°ë³¸ ê²€ìƒ‰ ì¸ë±ìŠ¤
            "CREATE INDEX IF NOT EXISTS idx_threat_posts_source_type ON threat_posts(source_type)",
            "CREATE INDEX IF NOT EXISTS idx_threat_posts_threat_type ON threat_posts(threat_type)",
            "CREATE INDEX IF NOT EXISTS idx_threat_posts_author ON threat_posts(author)",
            "CREATE INDEX IF NOT EXISTS idx_threat_posts_found_at ON threat_posts(found_at)",
            "CREATE INDEX IF NOT EXISTS idx_threat_posts_data_hash ON threat_posts(data_hash)",
            
            # IOC ê²€ìƒ‰ ìµœì í™” ì¸ë±ìŠ¤
            "CREATE INDEX IF NOT EXISTS idx_threat_iocs_value ON threat_iocs(ioc_value)",
            "CREATE INDEX IF NOT EXISTS idx_threat_iocs_type ON threat_iocs(ioc_type)",
            "CREATE INDEX IF NOT EXISTS idx_threat_iocs_post_id ON threat_iocs(post_id)",
            "CREATE INDEX IF NOT EXISTS idx_threat_iocs_value_type ON threat_iocs(ioc_value, ioc_type)",
            
            # ì—°ê´€ê´€ê³„ ê²€ìƒ‰ ìµœì í™” ì¸ë±ìŠ¤
            "CREATE INDEX IF NOT EXISTS idx_post_relationships_post1 ON post_relationships(post_id_1)",
            "CREATE INDEX IF NOT EXISTS idx_post_relationships_post2 ON post_relationships(post_id_2)",
            "CREATE INDEX IF NOT EXISTS idx_post_relationships_type ON post_relationships(relationship_type)",
            "CREATE INDEX IF NOT EXISTS idx_post_relationships_similarity ON post_relationships(similarity_score)"
        ]
        
        for index_sql in indexes:
            try:
                cursor.execute(index_sql)
            except sqlite3.OperationalError as e:
                if "already exists" not in str(e):
                    self.logger.warning(f"ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        
        conn.commit()
        conn.close()
        self.logger.info(f"ê³ ê¸‰ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì™„ë£Œ: {self.db_path}")

    # ==========================================================================
    # IOC(ìœ„í˜‘ì§€í‘œ) ì¶”ì¶œ ì‹œìŠ¤í…œ
    # ==========================================================================
    
    def extract_threat_indicators(self, text: str, title: str = "") -> Dict[str, List[Dict]]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ë‹¤ì–‘í•œ ìœ„í˜‘ ì§€í‘œ(IOC) ì¶”ì¶œ
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸ ë‚´ìš©
            title: ê²Œì‹œë¬¼ ì œëª© (ì„ íƒì )
            
        Returns:
            IOC íƒ€ì…ë³„ë¡œ ë¶„ë¥˜ëœ ë”•ì…”ë„ˆë¦¬
            ê° IOCëŠ” value, context, position ì •ë³´ë¥¼ í¬í•¨
        """
        full_text = f"{title} {text}".strip()
        
        # IOC ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
        indicators = {
            'emails': [],           # ì´ë©”ì¼ ì£¼ì†Œ
            'ips': [],              # IP ì£¼ì†Œ
            'domains': [],          # ë„ë©”ì¸ëª…
            'urls': [],             # URL
            'file_hashes': [],      # íŒŒì¼ í•´ì‹œê°’
            'crypto_addresses': [], # ì•”í˜¸í™”í ì£¼ì†Œ
            'leaked_accounts': [],  # ìœ ì¶œëœ ê³„ì •ëª…
            'phone_numbers': [],    # ì „í™”ë²ˆí˜¸
            'personal_names': [],    # ê°œì¸ ì´ë¦„ 
        }
        
        if not full_text:
            return indicators
        
        # 1. ì´ë©”ì¼ ì£¼ì†Œ ì¶”ì¶œ (ë‹¤ì–‘í•œ ë‚œë…í™” íŒ¨í„´ ì§€ì›)
        email_patterns = [
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',           # ì¼ë°˜ ì´ë©”ì¼
            r'\b[A-Za-z0-9._%+-]+\s*\[at\]\s*[A-Za-z0-9.-]+\s*\[dot\]\s*[A-Z|a-z]{2,}\b',  # [at], [dot] ë‚œë…í™”
            r'\b[A-Za-z0-9._%+-]+\s*\(@\)\s*[A-Za-z0-9.-]+\s*\(\.\)\s*[A-Z|a-z]{2,}\b'     # (@), (.) ë‚œë…í™”
        ]
        
        for pattern in email_patterns:
            for match in re.finditer(pattern, full_text, re.IGNORECASE):
                email = match.group()
                # ë‚œë…í™” í•´ì œ
                cleaned_email = email.replace('[at]', '@').replace('[dot]', '.').replace('(@)', '@').replace('(.)', '.')
                cleaned_email = re.sub(r'\s+', '', cleaned_email).lower()
                
                context = self.extract_context(full_text, match.start(), match.end())
                indicators['emails'].append({
                    'value': cleaned_email,
                    'context': context,
                    'position': match.start(),
                    'original_format': email  # ì›ë³¸ ë‚œë…í™” í˜•íƒœ ë³´ì¡´
                })
        
        # 2. IP ì£¼ì†Œ ì¶”ì¶œ (IPv4)
        ip_pattern = r'\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b'
        for match in re.finditer(ip_pattern, full_text):
            ip = match.group()
            # ì‚¬ì„¤ IP ì œì™¸ ì˜µì…˜ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
            # if self.is_private_ip(ip):
            #     continue
            context = self.extract_context(full_text, match.start(), match.end())
            indicators['ips'].append({
                'value': ip,
                'context': context,
                'position': match.start()
            })
        
        # 3. ë„ë©”ì¸ëª… ì¶”ì¶œ
        domain_pattern = r'\b(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}\b'
        for match in re.finditer(domain_pattern, full_text.lower()):
            domain = match.group()
            # ì´ë©”ì¼ ë„ë©”ì¸ê³¼ ì¤‘ë³µ ì œê±°
            if not any(email_item['value'].endswith(domain) for email_item in indicators['emails']):
                context = self.extract_context(full_text, match.start(), match.end())
                indicators['domains'].append({
                    'value': domain,
                    'context': context,
                    'position': match.start()
                })
        
        # 4. URL ì¶”ì¶œ
        url_patterns = [
            r'https?://[^\s<>"\'{}|\\^`\[\]]+',    # HTTP/HTTPS URL
            r'ftp://[^\s<>"\'{}|\\^`\[\]]+',       # FTP URL
            r'www\.[^\s<>"\'{}|\\^`\[\]]+'         # www.ë¡œ ì‹œì‘í•˜ëŠ” URL
        ]
        
        for pattern in url_patterns:
            for match in re.finditer(pattern, full_text):
                url = match.group()
                context = self.extract_context(full_text, match.start(), match.end())
                indicators['urls'].append({
                    'value': url,
                    'context': context,
                    'position': match.start()
                })
        
        # 5. íŒŒì¼ í•´ì‹œê°’ ì¶”ì¶œ (MD5, SHA1, SHA256, SHA512)
        hash_patterns = [
            (r'\b[a-fA-F0-9]{32}\b', 'md5'),      # MD5
            (r'\b[a-fA-F0-9]{40}\b', 'sha1'),     # SHA1
            (r'\b[a-fA-F0-9]{64}\b', 'sha256'),   # SHA256
            (r'\b[a-fA-F0-9]{128}\b', 'sha512')   # SHA512
        ]
        
        for pattern, hash_type in hash_patterns:
            for match in re.finditer(pattern, full_text):
                hash_value = match.group().lower()
                context = self.extract_context(full_text, match.start(), match.end())
                indicators['file_hashes'].append({
                    'value': hash_value,
                    'context': context,
                    'position': match.start(),
                    'hash_type': hash_type
                })
        
        # 6. ì•”í˜¸í™”í ì£¼ì†Œ ì¶”ì¶œ
        crypto_patterns = [
            (r'\b[13][a-km-zA-HJ-NP-Z1-9]{25,34}\b', 'bitcoin'),     # ë¹„íŠ¸ì½”ì¸
            (r'\b0x[a-fA-F0-9]{40}\b', 'ethereum'),                  # ì´ë”ë¦¬ì›€
            (r'\b[LM3][a-km-zA-HJ-NP-Z1-9]{26,33}\b', 'litecoin')    # ë¼ì´íŠ¸ì½”ì¸
        ]
        
        for pattern, crypto_type in crypto_patterns:
            for match in re.finditer(pattern, full_text):
                crypto_addr = match.group()
                context = self.extract_context(full_text, match.start(), match.end())
                indicators['crypto_addresses'].append({
                    'value': crypto_addr,
                    'context': context,
                    'position': match.start(),
                    'crypto_type': crypto_type
                })
        
        # 7. ìœ ì¶œëœ ê³„ì • ì •ë³´ ì¶”ì¶œ
        account_patterns = [
            r'(?:user|username|login|account|id)[:\s=]+([a-zA-Z0-9._-]{3,})',         # user: username í˜•íƒœ
            r'(?:admin|administrator)[:\s=]+([a-zA-Z0-9._-]{3,})',                   # admin: username í˜•íƒœ
            r'([a-zA-Z0-9._-]{3,})\s*:\s*[^\s\n]{3,}',                               # username:password í˜•íƒœ
            r'(?:credential|cred)[:\s]+([a-zA-Z0-9._-]{3,})'                         # credential: username í˜•íƒœ
        ]
        
        for pattern in account_patterns:
            for match in re.finditer(pattern, full_text, re.IGNORECASE):
                account = match.group(1) if match.groups() else match.group()
                # ì´ë©”ì¼ì˜ ì‚¬ìš©ìëª… ë¶€ë¶„ ì œì™¸
                if '@' not in account and len(account) >= 3:
                    context = self.extract_context(full_text, match.start(), match.end())
                    indicators['leaked_accounts'].append({
                        'value': account,
                        'context': context,
                        'position': match.start()
                    })
        
        # 8. ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
        phone_patterns = [
            r'\+?[1-9]\d{1,14}',                    # êµ­ì œ í˜•ì‹
            r'\b\d{3}[-.\s]?\d{3}[-.\s]?\d{4}\b',   # ë¯¸êµ­ í˜•ì‹
            r'\b010[-.\s]?\d{4}[-.\s]?\d{4}\b'      # í•œêµ­ í˜•ì‹
        ]
        
        for pattern in phone_patterns:
            for match in re.finditer(pattern, full_text):
                phone = match.group()
                context = self.extract_context(full_text, match.start(), match.end())
                indicators['phone_numbers'].append({
                    'value': phone,
                    'context': context,
                    'position': match.start()
                })
        
        # ğŸ†• PII ë°ì´í„°ì—ì„œ IOC ì¶”ì¶œ (textì—ì„œ PII íŒ¨í„´ ì°¾ê¸°)
        # PII íŒ¨í„´: [PII: Name: Jane Doe | Username: j.doe | Email: jane.doe@example.com]
        pii_pattern = r'\[PII:\s*([^\]]+)\]'
        pii_match = re.search(pii_pattern, full_text)
    
        if pii_match:
            pii_content = pii_match.group(1)
        
            # Name ì¶”ì¶œ
            name_pattern = r'Name:\s*([^|]+?)(?:\s*\||$)'
            name_match = re.search(name_pattern, pii_content)
            if name_match:
                name = name_match.group(1).strip()
                indicators['personal_names'].append({
                    'value': name,
                    'context': 'PII ë°ì´í„°ì—ì„œ ì¶”ì¶œëœ ê°œì¸ ì´ë¦„',
                    'position': pii_match.start(),
                    'confidence': 0.9,
                    'source': 'pii'
                })
        
            # Username ì¶”ì¶œ
            username_pattern = r'Username:\s*([^|]+?)(?:\s*\||$)'
            username_match = re.search(username_pattern, pii_content)
            if username_match:
                username = username_match.group(1).strip()
                indicators['leaked_accounts'].append({
                    'value': username,
                    'context': 'PII ë°ì´í„°ì—ì„œ ì¶”ì¶œëœ ê³„ì •ëª…',
                    'position': pii_match.start(),
                    'confidence': 0.95,
                    'source': 'pii'
                })
            
            # Email ì¶”ì¶œ
            email_pattern = r'Email:\s*([^|]+?)(?:\s*\||$)'
            email_match = re.search(email_pattern, pii_content)
            if email_match:
                email = email_match.group(1).strip().lower()
                indicators['emails'].append({
                    'value': email,
                    'context': 'PII ë°ì´í„°ì—ì„œ ì¶”ì¶œëœ ì´ë©”ì¼',
                    'position': pii_match.start(),
                    'confidence': 1.0,
                    'source': 'pii'
                })
            
            # Phone ì¶”ì¶œ
            phone_pattern = r'Phone:\s*([^|]+?)(?:\s*\||$)'
            phone_match = re.search(phone_pattern, pii_content)
            if phone_match:
                phone = phone_match.group(1).strip()
                indicators['phone_numbers'].append({
                    'value': phone,
                    'context': 'PII ë°ì´í„°ì—ì„œ ì¶”ì¶œëœ ì „í™”ë²ˆí˜¸',
                    'position': pii_match.start(),
                    'confidence': 0.8,
                    'source': 'pii'
                })
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        for ioc_type in indicators:
            # ê°’ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
            seen_values = set()
            unique_indicators = []
            for item in indicators[ioc_type]:
                if item['value'] not in seen_values:
                    seen_values.add(item['value'])
                    unique_indicators.append(item)
            
            # ìœ„ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
            indicators[ioc_type] = sorted(unique_indicators, key=lambda x: x['position'])
        
        self.logger.info(f"IOC ì¶”ì¶œ ì™„ë£Œ: {sum(len(v) for v in indicators.values())}ê°œ ì§€í‘œ ë°œê²¬")
        return indicators
    
    def extract_context(self, text: str, start: int, end: int, context_length: int = 80) -> str:
        """
        IOC ì£¼ë³€ì˜ ë§¥ë½ ì •ë³´ ì¶”ì¶œ
        
        Args:
            text: ì „ì²´ í…ìŠ¤íŠ¸
            start: IOC ì‹œì‘ ìœ„ì¹˜
            end: IOC ë ìœ„ì¹˜
            context_length: ì•ë’¤ë¡œ ì¶”ì¶œí•  ë¬¸ì ìˆ˜
            
        Returns:
            IOC ì£¼ë³€ ë§¥ë½ ë¬¸ìì—´
        """
        context_start = max(0, start - context_length)
        context_end = min(len(text), end + context_length)
        context = text[context_start:context_end].strip()
        
        # ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì—°ì† ê³µë°± ì œê±°
        context = re.sub(r'\s+', ' ', context)
        
        return context

    # ==========================================================================
    # ìœ ì‚¬ë„ ê³„ì‚° ë° ì—°ê´€ê´€ê³„ ë¶„ì„
    # ==========================================================================
    
    def calculate_content_similarity(self, data1: Dict, data2: Dict) -> float:
        """
        ë‘ ê²Œì‹œë¬¼ ê°„ì˜ ì „ì²´ ì½˜í…ì¸  ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            data1, data2: ë¹„êµí•  ê²Œì‹œë¬¼ ë°ì´í„°
            
        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        # í•„ë“œë³„ ê°€ì¤‘ì¹˜ ì„¤ì •
        field_weights = {
            'title': 0.3,     # ì œëª© 30%
            'text': 0.5,      # ë‚´ìš© 50%
            'author': 0.2     # ì‘ì„±ì 20%
        }
        
        total_similarity = 0.0
        total_weight = 0.0
        
        for field, weight in field_weights.items():
            text1 = str(data1.get(field, '')).strip()
            text2 = str(data2.get(field, '')).strip()
            
            if text1 and text2:
                # ë‘ ê°€ì§€ ìœ ì‚¬ë„ ì¸¡ì • ë°©ë²• ì‚¬ìš©
                
                # 1. ë¬¸ì ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ (í¸ì§‘ ê±°ë¦¬ ê¸°ë°˜)
                sequence_similarity = SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
                
                # 2. TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜)
                try:
                    vectorizer = TfidfVectorizer(
                        stop_words='english',  # ì˜ì–´ ë¶ˆìš©ì–´ ì œê±°
                        ngram_range=(1, 2),    # 1-gram, 2-gram ì‚¬ìš©
                        max_features=1000      # ìµœëŒ€ íŠ¹ì„± ìˆ˜ ì œí•œ
                    )
                    tfidf_matrix = vectorizer.fit_transform([text1, text2])
                    tfidf_similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
                except Exception as e:
                    # TF-IDF ê³„ì‚° ì‹¤íŒ¨ì‹œ ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ë§Œ ì‚¬ìš©
                    self.logger.warning(f"TF-IDF ê³„ì‚° ì‹¤íŒ¨: {e}")
                    tfidf_similarity = sequence_similarity
                
                # ë‘ ë°©ë²•ì˜ ê°€ì¤‘ í‰ê·  (ì‹œí€€ìŠ¤ 40%, TF-IDF 60%)
                field_similarity = (sequence_similarity * 0.4) + (tfidf_similarity * 0.6)
                
                total_similarity += field_similarity * weight
                total_weight += weight
        
        final_similarity = total_similarity / total_weight if total_weight > 0 else 0.0
        return min(1.0, max(0.0, final_similarity))  # 0.0 ~ 1.0 ë²”ìœ„ë¡œ ì œí•œ
    
    def calculate_ioc_difference_ratio(self, iocs1: Dict, iocs2: Dict) -> float:
        """
        ë‘ ê²Œì‹œë¬¼ì˜ IOC ì°¨ì´ ë¹„ìœ¨ ê³„ì‚°
        
        Args:
            iocs1, iocs2: ë¹„êµí•  IOC ë”•ì…”ë„ˆë¦¬
            
        Returns:
            IOC ì°¨ì´ ë¹„ìœ¨ (0.0 = ì™„ì „ ë™ì¼, 1.0 = ì™„ì „ ë‹¤ë¦„)
        """
        total_difference = 0.0
        field_count = 0
        
        # ì£¼ìš” IOC íƒ€ì…ë“¤ë§Œ ë¹„êµ
        important_ioc_types = ['emails', 'ips', 'file_hashes', 'leaked_accounts', 'crypto_addresses']
        
        for ioc_type in important_ioc_types:
            values1 = set([item['value'] for item in iocs1.get(ioc_type, [])])
            values2 = set([item['value'] for item in iocs2.get(ioc_type, [])])
            
            if values1 or values2:  # ì ì–´ë„ í•˜ë‚˜ì— ê°’ì´ ìˆëŠ” ê²½ìš°ë§Œ
                intersection = len(values1 & values2)  # êµì§‘í•©
                union = len(values1 | values2)         # í•©ì§‘í•©
                
                # Jaccard ê±°ë¦¬ ê³„ì‚° (1 - Jaccard ìœ ì‚¬ë„)
                difference = 1 - (intersection / union) if union > 0 else 1
                total_difference += difference
                field_count += 1
        
        return total_difference / field_count if field_count > 0 else 0.0

    # ==========================================================================
    # ì—°ê´€ê´€ê³„ ê¸°ë°˜ ë°ì´í„° ì €ì¥ ì‹œìŠ¤í…œ
    # ==========================================================================
    
    def save_with_relationship_detection(self, normalized_data: List[Dict]) -> Dict[str, int]:
        """
        ì—°ê´€ê´€ê³„ ê°ì§€ë¥¼ í¬í•¨í•œ ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ì €ì¥
        
        Args:
            normalized_data: ì •ê·œí™”ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì²˜ë¦¬ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        stats = {
            'inserted': 0,              # ìƒˆë¡œ ì €ì¥ëœ ë…ë¦½ ê²Œì‹œë¬¼
            'related_created': 0,       # ì—°ê´€ê´€ê³„ì™€ í•¨ê»˜ ì €ì¥ëœ ê²Œì‹œë¬¼
            'exact_duplicates': 0,      # ì™„ì „ ì¤‘ë³µìœ¼ë¡œ ë¬´ì‹œëœ ê²Œì‹œë¬¼
            'errors': 0                 # ì˜¤ë¥˜ ë°œìƒ ê±´ìˆ˜
        }
        
        batch_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        start_time = datetime.now()
        
        for i, item in enumerate(normalized_data):
            try:
                self.logger.info(f"ì²˜ë¦¬ ì¤‘: {i+1}/{len(normalized_data)} - {item.get('title', 'Unknown')[:50]}...")
                
                # 1. IOC ì¶”ì¶œ
                iocs = self.extract_threat_indicators(
                    item.get('text', ''), 
                    item.get('title', '')
                )
                
                # 2. ì™„ì „ ì¤‘ë³µ ì²´í¬
                if self.is_exact_duplicate(item):
                    stats['exact_duplicates'] += 1
                    self.logger.info("ì™„ì „ ì¤‘ë³µìœ¼ë¡œ ê±´ë„ˆëœ€")
                    continue
                
                # 3. ìœ ì‚¬í•œ ê¸°ì¡´ ê²Œì‹œë¬¼ ê²€ìƒ‰
                similar_posts = self.find_similar_existing_posts(item, iocs)
                
                if similar_posts:
                    # 4-1. ì—°ê´€ê´€ê³„ë¥¼ í¬í•¨í•œ ì €ì¥
                    post_id = self.save_new_post_with_relations(item, iocs, similar_posts)
                    if post_id:
                        stats['related_created'] += 1
                        self.logger.info(f"ì—°ê´€ê´€ê³„ í¬í•¨ ì €ì¥ ì™„ë£Œ: {len(similar_posts)}ê°œ ê´€ë ¨ ê²Œì‹œë¬¼")
                else:
                    # 4-2. ë…ë¦½ì ì¸ ìƒˆ ê²Œì‹œë¬¼ ì €ì¥
                    post_id = self.save_new_post(item, iocs)
                    if post_id:
                        stats['inserted'] += 1
                        self.logger.info("ìƒˆ ë…ë¦½ ê²Œì‹œë¬¼ ì €ì¥ ì™„ë£Œ")
                        
            except Exception as e:
                stats['errors'] += 1
                self.logger.error(f"ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
        
        # 5. ì²˜ë¦¬ í†µê³„ ì €ì¥
        processing_time = (datetime.now() - start_time).total_seconds()
        self.save_processing_statistics(batch_id, stats, processing_time, normalized_data)
        
        self.logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {stats}")
        return stats
    
    def find_similar_existing_posts(self, new_data: Dict, new_iocs: Dict, 
                                   similarity_threshold: float = 0.8,
                                   ioc_difference_threshold: float = 0.3,
                                   days_back: int = 30) -> List[Tuple[str, float]]:
        """
        ìƒˆ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ê¸°ì¡´ ê²Œì‹œë¬¼ ê²€ìƒ‰
        
        Args:
            new_data: ìƒˆë¡œìš´ ê²Œì‹œë¬¼ ë°ì´í„°
            new_iocs: ìƒˆë¡œìš´ ê²Œì‹œë¬¼ì˜ IOC
            similarity_threshold: ì½˜í…ì¸  ìœ ì‚¬ë„ ì„ê³„ê°’
            ioc_difference_threshold: IOC ì°¨ì´ ì„ê³„ê°’
            days_back: ê²€ìƒ‰í•  ê³¼ê±° ì¼ìˆ˜
            
        Returns:
            (ê²Œì‹œë¬¼_ID, ìœ ì‚¬ë„_ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # ê°™ì€ ì†ŒìŠ¤ íƒ€ì…ì˜ ìµœê·¼ ê²Œì‹œë¬¼ë“¤ ì¡°íšŒ (ì„±ëŠ¥ ìµœì í™”)
            cursor.execute('''
                SELECT id, title, text, author, found_at FROM threat_posts 
                WHERE source_type = ? 
                AND created_at > datetime('now', '-{} days')
                AND author = ?
                ORDER BY created_at DESC
                LIMIT 1000
            '''.format(days_back), (new_data.get('source_type', ''), new_data.get('author', '')))
            
            existing_posts = cursor.fetchall()
            self.logger.info(f"ë¹„êµ ëŒ€ìƒ ê²Œì‹œë¬¼: {len(existing_posts)}ê°œ")
            
            similar_posts = []
            
            for post_id, title, text, author, found_at in existing_posts:
                existing_data = {
                    'title': title or '',
                    'text': text or '',
                    'author': author or ''
                }
                
                # 1. ì½˜í…ì¸  ìœ ì‚¬ë„ ê³„ì‚°
                content_similarity = self.calculate_content_similarity(new_data, existing_data)
                
                if content_similarity >= similarity_threshold:
                    # 2. IOC ì°¨ì´ ê³„ì‚°
                    existing_iocs = self.get_post_iocs(post_id)
                    ioc_difference = self.calculate_ioc_difference_ratio(new_iocs, existing_iocs)
                    
                    if ioc_difference >= ioc_difference_threshold:
                        # ìœ ì‚¬í•˜ì§€ë§Œ IOCê°€ ì¶©ë¶„íˆ ë‹¤ë¦„ - ì—°ê´€ê´€ê³„ í›„ë³´
                        similar_posts.append((post_id, content_similarity))
                        self.logger.info(f"ì—°ê´€ê´€ê³„ í›„ë³´ ë°œê²¬: ID={post_id}, ìœ ì‚¬ë„={content_similarity:.3f}, IOCì°¨ì´={ioc_difference:.3f}")
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ìˆœ)
            similar_posts.sort(key=lambda x: x[1], reverse=True)
            
            return similar_posts[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ë°˜í™˜
            
        except Exception as e:
            self.logger.error(f"ìœ ì‚¬ ê²Œì‹œë¬¼ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
        finally:
            conn.close()
    
    def save_new_post_with_relations(self, data: Dict, iocs: Dict, 
                                   similar_posts: List[Tuple[str, float]]) -> Optional[str]:
        """
        ì—°ê´€ê´€ê³„ì™€ í•¨ê»˜ ìƒˆ ê²Œì‹œë¬¼ ì €ì¥
        
        Args:
            data: ê²Œì‹œë¬¼ ë°ì´í„°
            iocs: ì¶”ì¶œëœ IOC ì •ë³´
            similar_posts: ìœ ì‚¬í•œ ê¸°ì¡´ ê²Œì‹œë¬¼ë“¤ [(post_id, similarity_score), ...]
            
        Returns:
            ì €ì¥ëœ ê²Œì‹œë¬¼ ID (ì„±ê³µì‹œ) ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # 1. ìƒˆ ê²Œì‹œë¬¼ ì €ì¥
            import uuid
            post_id = str(uuid.uuid4())
            data_hash = self.generate_data_hash(data)
            local_time = self.get_local_time()

            cursor.execute('''
                INSERT INTO threat_posts 
                (id, source_type, thread_id, url, keyword, found_at, title, text, 
                 author, date, threat_type, platform, data_hash, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                post_id, data.get('source_type', ''), data.get('thread_id', ''), 
                data.get('url', ''), data.get('keyword', ''), data.get('found_at', ''),
                data.get('title', ''), data.get('text', ''), data.get('author', ''),
                data.get('date', ''), data.get('threat_type', ''), data.get('platform', ''),
                data_hash, local_time
            ))
            
            # 2. IOC ì •ë³´ ì €ì¥
            self.save_post_iocs(cursor, post_id, iocs)
            
            # 3. ì—°ê´€ê´€ê³„ ì €ì¥
            for similar_post_id, similarity_score in similar_posts:
                # ì–‘ë°©í–¥ ê´€ê³„ ìƒì„± ë°©ì§€ë¥¼ ìœ„í•´ ID ìˆœì„œ ì •ë ¬
                if post_id < similar_post_id:
                    pid1, pid2 = post_id, similar_post_id
                else:
                    pid1, pid2 = similar_post_id, post_id
                
                cursor.execute('''
                    INSERT OR IGNORE INTO post_relationships 
                    (post_id_1, post_id_2, relationship_type, similarity_score, description)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    pid1, pid2, 
                    'similar_content_different_ioc', 
                    similarity_score,
                    f"ìœ ì‚¬í•œ ë‚´ìš©ì´ì§€ë§Œ ì„œë¡œ ë‹¤ë¥¸ IOCë¥¼ í¬í•¨í•˜ëŠ” ê²Œì‹œë¬¼ë“¤"
                ))
            
            conn.commit()
            self.logger.info(f"ì—°ê´€ê´€ê³„ í¬í•¨ ê²Œì‹œë¬¼ ì €ì¥ ì™„ë£Œ: {post_id} (ì—°ê´€: {len(similar_posts)}ê°œ)")
            return post_id
            
        except Exception as e:
            conn.rollback()
            self.logger.error(f"ì—°ê´€ê´€ê³„ ì €ì¥ ì˜¤ë¥˜: {e}")
            return None
        finally:
            conn.close()
    
    def save_new_post(self, data: Dict, iocs: Dict) -> Optional[str]:
        """
        ë…ë¦½ì ì¸ ìƒˆ ê²Œì‹œë¬¼ ì €ì¥
        
        Args:
            data: ê²Œì‹œë¬¼ ë°ì´í„°
            iocs: ì¶”ì¶œëœ IOC ì •ë³´
            
        Returns:
            ì €ì¥ëœ ê²Œì‹œë¬¼ ID (ì„±ê³µì‹œ) ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            import uuid
            post_id = str(uuid.uuid4())
            data_hash = self.generate_data_hash(data)
            local_time = self.get_local_time()

            cursor.execute('''
                INSERT INTO threat_posts 
                (id, source_type, thread_id, url, keyword, found_at, title, text, 
                 author, date, threat_type, platform, data_hash, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                post_id, data.get('source_type', ''), data.get('thread_id', ''), 
                data.get('url', ''), data.get('keyword', ''), data.get('found_at', ''),
                data.get('title', ''), data.get('text', ''), data.get('author', ''),
                data.get('date', ''), data.get('threat_type', ''), data.get('platform', ''),
                data_hash, local_time
            ))
            
            # IOC ì •ë³´ ì €ì¥
            self.save_post_iocs(cursor, post_id, iocs)
            
            conn.commit()
            self.logger.info(f"ìƒˆ ë…ë¦½ ê²Œì‹œë¬¼ ì €ì¥ ì™„ë£Œ: {post_id}")
            return post_id
            
        except Exception as e:
            conn.rollback()
            self.logger.error(f"ê²Œì‹œë¬¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            return None
        finally:
            conn.close()
    
    def save_post_iocs(self, cursor, post_id: str, iocs: Dict):
        """
        ê²Œì‹œë¬¼ì˜ IOC ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        
        Args:
            cursor: ë°ì´í„°ë² ì´ìŠ¤ ì»¤ì„œ
            post_id: ê²Œì‹œë¬¼ ID
            iocs: IOC ë”•ì…”ë„ˆë¦¬
        """
        local_time = self.get_local_time()

        # IOC íƒ€ì… ë§¤í•‘ (ë‚´ë¶€ í‚¤ -> DB ì €ì¥ìš© íƒ€ì…ëª…)
        ioc_type_mapping = {
            'emails': 'email_address',
            'ips': 'ip_address', 
            'domains': 'domain',
            'urls': 'url',
            'file_hashes': 'file_hash',
            'crypto_addresses': 'crypto_address',
            'leaked_accounts': 'leaked_account',
            'phone_numbers': 'phone_number',
            'personal_names': 'personal_name'  
        }
        
        ioc_count = 0
        for ioc_category, db_type in ioc_type_mapping.items():
            for ioc_item in iocs.get(ioc_category, []):
                cursor.execute('''
                    INSERT INTO threat_iocs (post_id, ioc_type, ioc_value, context, confidence, first_seen)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    post_id, 
                    db_type, 
                    ioc_item['value'], 
                    ioc_item.get('context', ''),
                    1.0,  # ê¸°ë³¸ ì‹ ë¢°ë„
                    local_time
                ))
                ioc_count += 1
        
        self.logger.debug(f"IOC ì €ì¥ ì™„ë£Œ: {ioc_count}ê°œ")

    # ==========================================================================
    # í—¬í¼ ë©”ì„œë“œë“¤
    # ==========================================================================
    
    def generate_data_hash(self, data: Dict[str, Any]) -> str:
        """
        ë°ì´í„° ì¤‘ë³µ ê²€ì‚¬ë¥¼ ìœ„í•œ í•´ì‹œ ìƒì„±
        
        Args:
            data: í•´ì‹œë¥¼ ìƒì„±í•  ë°ì´í„°
            
        Returns:
            MD5 í•´ì‹œ ë¬¸ìì—´
        """
        # ì¤‘ë³µ ê²€ì‚¬ì— ì‚¬ìš©í•  í•µì‹¬ í•„ë“œë“¤
        key_fields = ['source_type', 'thread_id', 'title', 'text', 'author']
        hash_content = ''.join([str(data.get(field, '')) for field in key_fields])
        return hashlib.md5(hash_content.encode('utf-8')).hexdigest()
    
    def is_exact_duplicate(self, data: Dict) -> bool:
        """
        ì™„ì „ ì¤‘ë³µ ë°ì´í„° ì²´í¬
        
        Args:
            data: ì²´í¬í•  ë°ì´í„°
            
        Returns:
            ì¤‘ë³µ ì—¬ë¶€ (True/False)
        """
        data_hash = self.generate_data_hash(data)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM threat_posts WHERE data_hash = ?', (data_hash,))
        count = cursor.fetchone()[0]
        
        conn.close()
        return count > 0
    
    def get_post_iocs(self, post_id: str) -> Dict[str, List[Dict]]:
        """
        íŠ¹ì • ê²Œì‹œë¬¼ì˜ IOC ì •ë³´ ì¡°íšŒ
        
        Args:
            post_id: ê²Œì‹œë¬¼ ID
            
        Returns:
            IOC ë”•ì…”ë„ˆë¦¬ (extract_threat_indicatorsì™€ ë™ì¼í•œ í˜•íƒœ)
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT ioc_type, ioc_value, context FROM threat_iocs 
            WHERE post_id = ?
        ''', (post_id,))
        
        ioc_records = cursor.fetchall()
        conn.close()
        
        # DB íƒ€ì…ëª…ì„ ë‚´ë¶€ í‚¤ë¡œ ì—­ë§¤í•‘
        type_reverse_mapping = {
            'email_address': 'emails',
            'ip_address': 'ips',
            'domain': 'domains',
            'url': 'urls',
            'file_hash': 'file_hashes',
            'crypto_address': 'crypto_addresses',
            'leaked_account': 'leaked_accounts',
            'phone_number': 'phone_numbers',
            'personal_name': 'personal_names'
        }
        
        iocs = {key: [] for key in type_reverse_mapping.values()}
        
        for ioc_type, ioc_value, context in ioc_records:
            internal_key = type_reverse_mapping.get(ioc_type, 'unknown')
            if internal_key != 'unknown':
                iocs[internal_key].append({
                    'value': ioc_value,
                    'context': context,
                    'position': 0  # DBì—ì„œ ì¡°íšŒì‹œ ìœ„ì¹˜ ì •ë³´ ì—†ìŒ
                })
        
        return iocs
    
    def save_processing_statistics(self, batch_id: str, stats: Dict, 
                                 processing_time: float, input_data: List):
        """
        ì²˜ë¦¬ í†µê³„ ì •ë³´ ì €ì¥
        
        Args:
            batch_id: ë°°ì¹˜ ì²˜ë¦¬ ID
            stats: ì²˜ë¦¬ í†µê³„
            processing_time: ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
            input_data: ì…ë ¥ ë°ì´í„°
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT INTO processing_statistics 
                (batch_id, total_input, new_posts, related_posts, duplicate_posts, 
                 error_count, processing_time_seconds)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                batch_id,
                len(input_data),
                stats['inserted'],
                stats['related_created'], 
                stats['exact_duplicates'],
                stats['errors'],
                processing_time
            ))
            
            conn.commit()
            self.logger.info(f"ì²˜ë¦¬ í†µê³„ ì €ì¥ ì™„ë£Œ: {batch_id}")
            
        except Exception as e:
            self.logger.error(f"í†µê³„ ì €ì¥ ì˜¤ë¥˜: {e}")
        finally:
            conn.close()

    # ==========================================================================
    # ê³ ê¸‰ ê²€ìƒ‰ ì‹œìŠ¤í…œ
    # ==========================================================================
    
    def search_ioc_with_relations(self, ioc_value: str, ioc_type: str = None) -> List[Dict]:
        """
        IOCë¡œ ê²€ìƒ‰í•˜ë˜ ì—°ê´€ ê²Œì‹œë¬¼ë„ í•¨ê»˜ ë°˜í™˜
        
        Args:
            ioc_value: ê²€ìƒ‰í•  IOC ê°’
            ioc_type: IOC íƒ€ì… (ì„ íƒì , ì—†ìœ¼ë©´ ëª¨ë“  íƒ€ì…ì—ì„œ ê²€ìƒ‰)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì—°ê´€ ê²Œì‹œë¬¼ ì •ë³´ í¬í•¨)
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # IOCê°€ í¬í•¨ëœ ê²Œì‹œë¬¼ ì§ì ‘ ê²€ìƒ‰
            if ioc_type:
                cursor.execute('''
                    SELECT DISTINCT p.id, p.title, p.text, p.author, p.found_at, 
                           p.source_type, p.threat_type, p.platform
                    FROM threat_posts p
                    JOIN threat_iocs i ON p.id = i.post_id
                    WHERE i.ioc_value = ? AND i.ioc_type = ?
                    ORDER BY p.found_at DESC
                ''', (ioc_value.lower(), ioc_type))
            else:
                cursor.execute('''
                    SELECT DISTINCT p.id, p.title, p.text, p.author, p.found_at,
                           p.source_type, p.threat_type, p.platform
                    FROM threat_posts p
                    JOIN threat_iocs i ON p.id = i.post_id
                    WHERE i.ioc_value = ?
                    ORDER BY p.found_at DESC
                ''', (ioc_value.lower(),))
            
            direct_matches = cursor.fetchall()
            
            results = []
            processed_posts = set()  # ì¤‘ë³µ ë°©ì§€
            
            for post_id, title, text, author, found_at, source_type, threat_type, platform in direct_matches:
                if post_id in processed_posts:
                    continue
                processed_posts.add(post_id)
                
                # ì—°ê´€ ê²Œì‹œë¬¼ ê²€ìƒ‰
                related_posts = self.get_related_posts(cursor, post_id)
                
                # í•´ë‹¹ ê²Œì‹œë¬¼ì˜ ëª¨ë“  IOC ì¡°íšŒ
                post_iocs = self.get_post_iocs(post_id)
                
                results.append({
                    'post_id': post_id,
                    'title': title,
                    'text': text,
                    'author': author,
                    'found_at': found_at,
                    'source_type': source_type,
                    'threat_type': threat_type,
                    'platform': platform,
                    'iocs': post_iocs,
                    'related_posts': related_posts,
                    'match_reason': f"IOC '{ioc_value}' ì§ì ‘ ë°œê²¬"
                })
            
            self.logger.info(f"IOC ê²€ìƒ‰ ì™„ë£Œ: '{ioc_value}' - {len(results)}ê°œ ê²°ê³¼")
            return results
            
        except Exception as e:
            self.logger.error(f"IOC ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
        finally:
            conn.close()
    
    def get_related_posts(self, cursor, post_id: str) -> List[Dict]:
        """
        íŠ¹ì • ê²Œì‹œë¬¼ì˜ ì—°ê´€ ê²Œì‹œë¬¼ë“¤ ì¡°íšŒ
        
        Args:
            cursor: ë°ì´í„°ë² ì´ìŠ¤ ì»¤ì„œ
            post_id: ê¸°ì¤€ ê²Œì‹œë¬¼ ID
            
        Returns:
            ì—°ê´€ ê²Œì‹œë¬¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        cursor.execute('''
            SELECT p.id, p.title, p.author, p.found_at, r.similarity_score, r.relationship_type
            FROM post_relationships r
            JOIN threat_posts p ON (
                CASE 
                    WHEN r.post_id_1 = ? THEN p.id = r.post_id_2
                    WHEN r.post_id_2 = ? THEN p.id = r.post_id_1
                    ELSE 0
                END
            )
            WHERE r.post_id_1 = ? OR r.post_id_2 = ?
            ORDER BY r.similarity_score DESC
        ''', (post_id, post_id, post_id, post_id))
        
        related_data = cursor.fetchall()
        
        related_posts = []
        for rel_id, rel_title, rel_author, rel_found_at, similarity, rel_type in related_data:
            related_posts.append({
                'id': rel_id,
                'title': rel_title,
                'author': rel_author,
                'found_at': rel_found_at,
                'similarity_score': similarity,
                'relationship_type': rel_type
            })
        
        return related_posts
    
    def search_by_author_with_timeline(self, author: str, days_back: int = 30) -> Dict:
        """
        ì‘ì„±ìë³„ í™œë™ íƒ€ì„ë¼ì¸ ê²€ìƒ‰
        
        Args:
            author: ê²€ìƒ‰í•  ì‘ì„±ìëª…
            days_back: ê²€ìƒ‰í•  ê³¼ê±° ì¼ìˆ˜
            
        Returns:
            ì‘ì„±ì í™œë™ ì •ë³´ ë° íƒ€ì„ë¼ì¸
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # ì‘ì„±ìì˜ ê²Œì‹œë¬¼ë“¤ ì¡°íšŒ
            cursor.execute('''
                SELECT id, title, found_at, threat_type, platform
                FROM threat_posts 
                WHERE author = ? 
                AND found_at > datetime('now', '-{} days')
                ORDER BY found_at DESC
            '''.format(days_back), (author,))
            
            posts = cursor.fetchall()
            
            if not posts:
                return {'author': author, 'posts': [], 'total_posts': 0, 'unique_iocs': 0}
            
            # ì‘ì„±ìì˜ ëª¨ë“  IOC ìˆ˜ì§‘
            all_iocs = set()
            post_details = []
            
            for post_id, title, found_at, threat_type, platform in posts:
                post_iocs = self.get_post_iocs(post_id)
                
                # ëª¨ë“  IOC ê°’ ìˆ˜ì§‘
                for ioc_list in post_iocs.values():
                    for ioc in ioc_list:
                        all_iocs.add(ioc['value'])
                
                post_details.append({
                    'id': post_id,
                    'title': title,
                    'found_at': found_at,
                    'threat_type': threat_type,
                    'platform': platform,
                    'ioc_count': sum(len(ioc_list) for ioc_list in post_iocs.values())
                })
            
            return {
                'author': author,
                'total_posts': len(posts),
                'unique_iocs': len(all_iocs),
                'date_range': f"{posts[-1][2]} ~ {posts[0][2]}",
                'posts': post_details
            }
            
        except Exception as e:
            self.logger.error(f"ì‘ì„±ì ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {'author': author, 'posts': [], 'total_posts': 0, 'unique_iocs': 0}
        finally:
            conn.close()
    
    def get_database_statistics(self) -> Dict:
        """
        ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ í†µê³„ ì¡°íšŒ
        
        Returns:
            í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            stats = {}
            
            # ê¸°ë³¸ í†µê³„
            cursor.execute("SELECT COUNT(*) FROM threat_posts")
            stats['total_posts'] = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM threat_iocs")
            stats['total_iocs'] = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM post_relationships")
            stats['total_relationships'] = cursor.fetchone()[0]
            
            # ì†ŒìŠ¤ë³„ ë¶„í¬
            cursor.execute("SELECT source_type, COUNT(*) FROM threat_posts GROUP BY source_type")
            stats['posts_by_source'] = dict(cursor.fetchall())
            
            # ìœ„í˜‘ ìœ í˜•ë³„ ë¶„í¬
            cursor.execute("SELECT threat_type, COUNT(*) FROM threat_posts GROUP BY threat_type")
            stats['posts_by_threat_type'] = dict(cursor.fetchall())
            
            # IOC íƒ€ì…ë³„ ë¶„í¬
            cursor.execute("SELECT ioc_type, COUNT(*) FROM threat_iocs GROUP BY ioc_type")
            stats['iocs_by_type'] = dict(cursor.fetchall())
            
            # í™œì„± ì‘ì„±ì TOP 10
            cursor.execute('''
                SELECT author, COUNT(*) as post_count 
                FROM threat_posts 
                WHERE author IS NOT NULL AND author != ''
                GROUP BY author 
                ORDER BY post_count DESC 
                LIMIT 10
            ''')
            stats['top_authors'] = dict(cursor.fetchall())
            
            # ìµœê·¼ í™œë™ í†µê³„ (7ì¼)
            cursor.execute('''
                SELECT COUNT(*) FROM threat_posts 
                WHERE created_at > datetime('now', '-7 days')
            ''')
            stats['posts_last_7_days'] = cursor.fetchone()[0]
            
            return stats
            
        except Exception as e:
            self.logger.error(f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {}
        finally:
            conn.close()












class MultiFormatThreatNormalizer:
    #JSON, CSV ë“± ë‹¤ì–‘í•œ í˜•ì‹ì˜ ìœ„í˜‘ì •ë³´ ë°ì´í„°ë¥¼ í‘œì¤€í™”í•˜ëŠ” í´ë˜ìŠ¤

    def __init__(self, output_folder: str = 'normalized_threat_data', db_path: str = 'threat_intelligence.db'):
        self.output_folder = output_folder
        self.db_path = db_path
        self.setup_logging() 
        self.create_output_folder()
        self.threat_processor = ThreatProcessingSystem(db_path)
        self.threat_processor.init_advanced_database()


        # í†µí•© í‘œì¤€ í•„ë“œ ì •ì˜
        self.standard_fields = {
            'source_type': '',      # 'telegram' ë˜ëŠ” 'darkweb'
            'thread_id': '',        # ê³ ìœ  ì‹ë³„ì
            'url': '',              # ë§í¬/URL
            'keyword': '',          # ê²€ìƒ‰ í‚¤ì›Œë“œ/íƒì§€ëœ í‚¤ì›Œë“œ
            'found_at': '',         # ë°œê²¬ ì‹œê°„
            'title': '',            # ì œëª©/ë©”ì‹œì§€ ìš”ì•½
            'text': '',             # ë‚´ìš©
            'author': '',           # ì‘ì„±ì/ì±„ë„ëª…
            'date': '',             # ì‘ì„±ì¼/íƒ€ì„ìŠ¤íƒ¬í”„
            'threat_type': '',      # ìœ„í˜‘ ìœ í˜•
            'platform': '',          # í”Œë«í¼ ì •ë³´ (í¬ëŸ¼ëª…/ì±„ë„ëª…)
            'event_id': '',       # ì´ë²¤íŠ¸ ID (ê³ ìœ  ì‹ë³„ì)
            'event_info': '',  # ì´ë²¤íŠ¸ ê´€ë ¨ ì •ë³´ (ì˜ˆ: í•´ì‹œ, IOC ë“±)
            'event_date': ''  # ì´ë²¤íŠ¸ ë°œìƒ ë‚ ì§œ
        }

        # ì†ŒìŠ¤ë³„ í•„ë“œ ë§¤í•‘ ì •ì˜
        self.field_mappings = self._create_field_mappings()

    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
        )
        self.logger = logging.getLogger(__name__)
    
    def create_output_folder(self):
        Path(self.output_folder).mkdir(parents=True, exist_ok=True)
        self.logger.info(f'ì¶œë ¥ í´ë” ìƒì„±: {self.output_folder}')
    
    def _create_field_mappings(self) -> Dict[str, List[str]]:
        #ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ í•„ë“œëª… ë§¤í•‘ ì •ì˜
        return {
            'thread_id': [
                'thread_id', 'Message ID', 'message_id', 'id', 'msg_id', 'post_id', 'event_id'
            ],
            'url': [
                'url', 'link', 'source_url', 'URL', 'Link'
            ],
            'keyword': [
                'keyword', 'Detected Keywords', 'detected_keywords', 
                'keywords', 'search_terms', 'query', 'Keywords'
            ],
            'found_at': [
                'found_at', 'Timestamp', 'timestamp', 'date_collected',
                'Found At', 'collected_at', 'crawled_at'
            ],
            'title': [
                'title', 'subject', 'headline', 'message_preview', 'Title','event_info'
            ],
            'text': [
                'text', 'Content', 'content', 'preview', 'message', 
                'description', 'hidden_content', 'Message', 'Text'
            ],
            'author': [
                'author', 'Channel', 'channel', 'username', 'user',
                'Author', 'User', 'Username', 'creator_org'
            ],
            'date': [
                'date', 'created_at', 'post_date', 'message_date',
                'Date', 'Created At', 'Post Date', 'event_date'
            ],
            'threat_type': [
                'threat_type', 'Threat Type', 'category', 'type',
                'Category', 'Type', 'threat_category'
            ],
            'platform': [
                'forum', 'platform', 'source_platform', 'site',
                'Forum', 'Platform', 'Source', 'sourcetype'
            ],
            'event_id': [
                'event_id', 'event_id', 'Event ID', 'id'
            ],
            'event_info': [
                'event_info', 'info', 'description'
            ],
            'event_date': [
                'event_date', 'event_timestamp'
            ]
        }

    def load_csv_data(self, file_path: str) -> List[Dict[str, Any]]:
        #CSV íŒŒì¼ì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë¡œë“œ
        try:
            # pandasë¥¼ ì‚¬ìš©í•˜ì—¬ CSV ë¡œë“œ (ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„)
            encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'latin-1']
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    self.logger.info(f"CSV ë¡œë“œ ì„±ê³µ (ì¸ì½”ë”©: {encoding}): {file_path}")
                    
                    # DataFrameì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    data = df.to_dict('records')
                    
                    # NaN ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜
                    for record in data:
                        for key, value in record.items():
                            if pd.isna(value):
                                record[key] = ""
                    
                    self.logger.info(f"CSV ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ ë ˆì½”ë“œ")
                    return data
                    
                except UnicodeDecodeError:
                    continue
                    
            raise Exception("ì§€ì›ë˜ëŠ” ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            self.logger.error(f"CSV íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {file_path}: {e}")
            return []

    def load_json_data(self, file_path: str) -> List[Dict[str, Any]]:
        #JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # JSON ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if isinstance(data, dict):
                if 'data' in data and isinstance(data['data'], list):
                    return data['data']
                else:
                    return [data]
            elif isinstance(data, list):
                return data
            else:
                return []
                
        except Exception as e:
            self.logger.error(f"JSON íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {file_path}: {e}")
            return []

    def detect_source_type(self, data: Dict[str, Any]) -> str:
        #ë°ì´í„° êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì†ŒìŠ¤ íƒ€ì… ìë™ ê°ì§€
        #MISP ê°ì§€ ë¡œì§
        if 'sourcetype' in data and data['sourcetype'] == 'MISP':
            return 'misp'
        if 'event_id' in data and 'creator_org' in data:
            return 'misp'
        if 'pii_data' in data and 'event_info' in data:
            return 'misp'
        
        # í…”ë ˆê·¸ë¨ ë°ì´í„° íŠ¹ì„± í™•ì¸
        telegram_indicators = [
            'Channel', 'Message ID', 'Threat Type', 'Detected Keywords',
            'channel', 'message_id', 'threat_type', 'detected_keywords'
        ]
        
        # ë‹¤í¬ì›¹ ë°ì´í„° íŠ¹ì„± í™•ì¸
        darkweb_indicators = [
            'forum', 'thread_id', 'has_hidden_content', 'preview',
            'Forum', 'Thread ID', 'author', 'url'
        ]
        
        telegram_score = sum(1 for indicator in telegram_indicators if indicator in data)
        darkweb_score = sum(1 for indicator in darkweb_indicators if indicator in data)
        
        if telegram_score > darkweb_score:
            return 'telegram'
        elif darkweb_score > telegram_score:
            return 'darkweb'
        else:
            # ê¸°ë³¸ê°’ ë˜ëŠ” ì¶”ê°€ íœ´ë¦¬ìŠ¤í‹± ì ìš©
            if any(indicator in data for indicator in ['Channel', 'Message ID', 'channel']):
                return 'telegram'
            elif any(indicator in data for indicator in ['forum', 'thread_id', 'Forum']):
                return 'darkweb'
            else:
                return 'unknown'

    def find_matching_field(self, data: Dict[str, Any], target_field: str) -> Optional[str]:
        #í‘œì¤€ í•„ë“œì— ë§¤í•‘ë˜ëŠ” ì›ë³¸ í•„ë“œëª… ì°¾ê¸°
        possible_names = self.field_mappings.get(target_field, [])

        for field_name in possible_names:
            if field_name in data:
                return field_name
            
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰
            for key in data.keys():
                if key.lower() == field_name.lower():
                    return key
        
        return None
    
    def clean_text(self, text: Any) -> str:
        #í…ìŠ¤íŠ¸ ì •ì œ ë° ì •ê·œí™”
        if text is None or pd.isna(text):
            return ""

        text_str = str(text).strip()
        
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜
        text_str = re.sub(r'\s+', ' ', text_str)
        # HTML íƒœê·¸ ì œê±°
        text_str = re.sub(r'<[^>]+>', '', text_str)
        # ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
        text_str = text_str.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')

        return text_str.strip()

    def normalize_single_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        #ë‹¨ì¼ ë°ì´í„° í•­ëª©ì„ í‘œì¤€ êµ¬ì¡°ë¡œ ë³€í™˜
        normalized = {}
        
        # ì†ŒìŠ¤ íƒ€ì… ìë™ ê°ì§€
        source_type = self.detect_source_type(item)
        normalized['source_type'] = source_type
        
        # ê° í‘œì¤€ í•„ë“œì— ëŒ€í•´ ë§¤í•‘ ìˆ˜í–‰
        for standard_field in self.standard_fields:
            if standard_field == 'source_type':
                continue  # ì´ë¯¸ ì„¤ì •ë¨
                
            original_field = self.find_matching_field(item, standard_field)
            
            if original_field:
                value = item[original_field]
            else:
                value = ""
            
            # íŠ¹ë³„ ì²˜ë¦¬ê°€ í•„ìš”í•œ í•„ë“œë“¤
            if standard_field in ['text', 'title']:
                normalized[standard_field] = self.clean_text(value)
            elif standard_field == 'keyword':
                # ì—¬ëŸ¬ í‚¤ì›Œë“œê°€ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ê²½ìš° ì²˜ë¦¬
                if value:
                    if isinstance(value, str):
                        keywords = value.split(',')
                        normalized[standard_field] = ', '.join([kw.strip() for kw in keywords])
                    else:
                        normalized[standard_field] = str(value).strip()
                else:
                    normalized[standard_field] = ""
            elif standard_field == 'date':
                # ë‚ ì§œ í˜•ì‹ í†µì¼
                normalized[standard_field] = self.normalize_date(str(value) if value else "")
            else:
                normalized[standard_field] = str(value).strip() if value and not pd.isna(value) else ""
        
        # ì†ŒìŠ¤ë³„ íŠ¹ë³„ ì²˜ë¦¬
        if source_type == 'telegram':
            # í…”ë ˆê·¸ë¨ì˜ ê²½ìš° ì±„ë„ëª…ì„ authorì™€ platform ë‘˜ ë‹¤ì— ì„¤ì •
            if normalized['author']:
                normalized['platform'] = normalized['author']
        elif source_type == 'darkweb':
            # ë‹¤í¬ì›¹ì˜ ê²½ìš° ìˆ¨ê²¨ì§„ ì½˜í…ì¸  ì •ë³´ ì¶”ê°€
            if 'has_hidden_content' in item and item['has_hidden_content']:
                hidden_content = item.get('hidden_content', '')
                if hidden_content:
                    normalized['text'] += f" [Hidden Content: {self.clean_text(hidden_content)}]"
        
        # thread_idê°€ ì—†ëŠ” ê²½ìš° ìƒì„±
        if not normalized['thread_id']:
            import uuid
            normalized['thread_id'] = str(uuid.uuid4())[:8]
            self.logger.warning(f"thread_id ìƒì„±: {normalized['thread_id']}")
        
        if source_type == 'misp':
        
            # 1. PII ë°ì´í„°ë¥¼ textì— ì¶”ê°€
            if 'pii_data' in item and item['pii_data']:
                pii = item['pii_data']
                pii_parts = []
            
                if pii.get('name'):
                    pii_parts.append(f"Name: {pii['name']}")
                if pii.get('username'):
                    pii_parts.append(f"Username: {pii['username']}")
                if pii.get('email'):
                    pii_parts.append(f"Email: {pii['email']}")
                if pii.get('password'):
                    pii_parts.append("Password: [REDACTED]")
                if pii.get('phone'):
                    pii_parts.append(f"Phone: {pii['phone']}")
            
                if pii_parts:
                    current_text = normalized.get('text', '')
                    pii_text = ' | '.join(pii_parts)
                    normalized['text'] = f"{current_text} [PII: {pii_text}]".strip()
        
            # 2. MISP ê¸°ë³¸ ì„¤ì •
            normalized['platform'] = 'MISP'
            normalized['threat_type'] = 'OSINT'
        
            # 3. event_infoë¥¼ title/textì— í™œìš© (ë¹„ì–´ìˆëŠ” ê²½ìš°)
            if item.get('event_info'):
                if not normalized.get('title'):
                    normalized['title'] = item['event_info']
                if not normalized.get('text') or normalized['text'] == '':
                    normalized['text'] = item['event_info']
        
        return normalized

    def normalize_date(self, date_str: str) -> str:
    
        if not date_str or pd.isna(date_str) or str(date_str).strip() == '':
            return datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ê¸°ë³¸ê°’
    
        date_str = str(date_str).strip()
    
    # ë‹¤ì–‘í•œ ë‚ ì§œ íŒ¨í„´ ì§€ì› (ê¸°ì¡´ë³´ë‹¤ í™•ì¥)
        patterns = [
            ('%Y-%m-%d %H:%M:%S', r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}'),
            ('%Y-%m-%d', r'\d{4}-\d{2}-\d{2}'),
            ('%m/%d/%Y', r'\d{1,2}/\d{1,2}/\d{4}'),
            ('%Y/%m/%d', r'\d{4}/\d{1,2}/\d{1,2}'),
            ('%d-%m-%Y', r'\d{1,2}-\d{1,2}-\d{4}'),
            ('%Y.%m.%d', r'\d{4}\.\d{1,2}\.\d{1,2}'),
        ]
    
        for date_format, pattern in patterns:
            if re.match(pattern, date_str):
                try:
                    parsed_date = datetime.strptime(date_str, date_format)
                    return parsed_date.strftime('%Y-%m-%d %H:%M:%S')
                except ValueError:
                    continue
    
    # íŒŒì‹± ì‹¤íŒ¨ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
        self.logger.warning(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨, í˜„ì¬ ì‹œê°„ ì‚¬ìš©: {date_str}")
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    def process_file(self, input_file: str, save_to_db: bool = False) -> Dict[str, Any]:
        """íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í‘œì¤€í™”ëœ ë°ì´í„° ìƒì„± (JSON, CSV ì§€ì›)"""
        try:
            file_extension = os.path.splitext(input_file)[1].lower()
            
            # íŒŒì¼ í˜•ì‹ì— ë”°ë¼ ë¡œë“œ ë°©ë²• ì„ íƒ
            if file_extension == '.csv':
                raw_data = self.load_csv_data(input_file)
                if not raw_data:
                    raise Exception("CSV íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            elif file_extension == '.json':
                raw_data = self.load_json_data(input_file)
                if not raw_data:
                    raise Exception("JSON íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                raise Exception(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}")

            normalized_items = []
            
            # ê° ë°ì´í„° í•­ëª© í‘œì¤€í™”
            for item in raw_data:
                if isinstance(item, dict):
                    normalized_item = self.normalize_single_item(item)
                    normalized_items.append(normalized_item)

            # ì¶œë ¥ íŒŒì¼ ìƒì„± (í•­ìƒ JSONìœ¼ë¡œ ì €ì¥)
            input_filename = os.path.splitext(os.path.basename(input_file))[0]
            output_filename = f"normalized_{input_filename}.json"
            output_path = os.path.join(self.output_folder, output_filename)

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(normalized_items, f, ensure_ascii=False, indent=2)

            result = {
                'status': 'SUCCESS',
                'input_file': input_file,
                'input_format': file_extension[1:],  # '.csv' -> 'csv'
                'output_file': output_path,
                'original_count': len(raw_data),
                'normalized_count': len(normalized_items),
                'detected_sources': list(set(item['source_type'] for item in normalized_items))
            }

            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            if save_to_db:
                db_result = self.save_to_database(normalized_items)
                result['database_saved'] = db_result['saved']
                result['database_duplicates'] = db_result['duplicates']
                result['database_errors'] = db_result['errors']

            self.logger.info(f'íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {input_file} -> {output_path}')
            return result
        
        except Exception as e:
            error_result = {
                'status': 'ERROR',
                'input_file': input_file,
                'error': str(e)
            }
            self.logger.error(f'íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {input_file}: {e}')
            return error_result

    def process_folder(self, input_folder: str, save_to_db: bool = False) -> List[Dict[str, Any]]:
        #í´ë” ë‚´ ëª¨ë“  JSON/CSV íŒŒì¼ ì¼ê´„ ì²˜ë¦¬
        # JSONê³¼ CSV íŒŒì¼ ëª¨ë‘ ì°¾ê¸°
        json_files = glob.glob(os.path.join(input_folder, "*.json"))
        csv_files = glob.glob(os.path.join(input_folder, "*.csv"))
        all_files = json_files + csv_files
        
        if not all_files:
            self.logger.warning(f"JSON/CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_folder}")
            return []
        
        self.logger.info(f"ì²˜ë¦¬ ì‹œì‘: {len(all_files)}ê°œ íŒŒì¼ (JSON: {len(json_files)}, CSV: {len(csv_files)})")
        
        results = []
        success_count = 0
        error_count = 0
        all_sources = set()
        format_stats = defaultdict(int)
        
        for file_path in all_files:
            result = self.process_file(file_path, save_to_db)
            results.append(result)
            
            if result['status'] == 'SUCCESS':
                success_count += 1
                all_sources.update(result['detected_sources'])
                format_stats[result['input_format']] += 1
            else:
                error_count += 1
        
        self.logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {error_count}ê°œ")
        self.logger.info(f"íŒŒì¼ í˜•ì‹ë³„ í†µê³„: {dict(format_stats)}")
        self.logger.info(f"ê°ì§€ëœ ì†ŒìŠ¤ íƒ€ì…: {', '.join(all_sources)}")
        
        return results
    
    def save_to_database(self, normalized_data: List[Dict[str, Any]]) -> Dict[str, int]:
    
        if not normalized_data:
            self.logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {'saved': 0, 'errors': 0}
    
        try:
            # ThreatProcessingSystemì˜ ì €ì¥ ë©”ì„œë“œ í˜¸ì¶œ
            stats = self.threat_processor.save_with_relationship_detection(normalized_data)
        
            self.logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {stats}")
            return {
                'saved': stats.get('inserted', 0) + stats.get('related_created', 0),
                'duplicates': stats.get('exact_duplicates', 0),
                'errors': stats.get('errors', 0)
            }
        
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            return {'saved': 0, 'errors': 1}

    def get_database_stats(self) -> Dict:
        #ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ
        return self.threat_processor.get_database_statistics()







def main():
    print("=== ë‹¤ì¤‘ í˜•ì‹ ìœ„í˜‘ì •ë³´ ë°ì´í„° í‘œì¤€í™” ë„êµ¬ ===")
    print("ì§€ì› í˜•ì‹: JSON, CSV")
    
    normalizer = MultiFormatThreatNormalizer()
    
    while True:
        print("\n1. í´ë” ì²˜ë¦¬ (JSON/CSV íŒŒì¼ë“¤ ì¼ê´„ ë³€í™˜)")
        print("2. ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬") 
        print("3. ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ")
        print("4. ì¢…ë£Œ")
        
        choice = input("ì„ íƒí•˜ì„¸ìš” (1-4): ").strip()
        
        if choice == "1":
            input_folder = input("ì…ë ¥ í´ë” ê²½ë¡œ: ").strip()
            save_to_db = input("ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower() == 'y'
            if not os.path.exists(input_folder):
                print(f"í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_folder}")
                continue
            
            results = normalizer.process_folder(input_folder, save_to_db)
            
            # ê²°ê³¼ ìš”ì•½
            success_results = [r for r in results if r['status'] == 'SUCCESS']
            error_results = [r for r in results if r['status'] == 'ERROR']
            
            print(f"\n=== ì²˜ë¦¬ ê²°ê³¼ ===")
            print(f"ì´ íŒŒì¼: {len(results)}ê°œ")
            print(f"ì„±ê³µ: {len(success_results)}ê°œ") 
            print(f"ì‹¤íŒ¨: {len(error_results)}ê°œ")
            print(f"ì¶œë ¥ í´ë”: {normalizer.output_folder}")
            
            # íŒŒì¼ í˜•ì‹ë³„ í†µê³„
            format_stats = defaultdict(int)
            for result in success_results:
                format_stats[result.get('input_format', 'unknown')] += 1
            
            if format_stats:
                print(f"ì²˜ë¦¬ëœ íŒŒì¼ í˜•ì‹: {dict(format_stats)}")
            
            # ì†ŒìŠ¤ íƒ€ì…ë³„ í†µê³„
            all_sources = set()
            for result in success_results:
                all_sources.update(result.get('detected_sources', []))
            
            if all_sources:
                print(f"ê°ì§€ëœ ë°ì´í„° ì†ŒìŠ¤: {', '.join(all_sources)}")
            
            if error_results:
                print("\nì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
                for error in error_results:
                    print(f"  - {error['input_file']}: {error['error']}")
        
        elif choice == "2":
            input_file = input("ì…ë ¥ íŒŒì¼ ê²½ë¡œ (JSON ë˜ëŠ” CSV): ").strip()
            save_to_db = input("ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower() == 'y'

            if not os.path.exists(input_file):
                print(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_file}")
                continue
            
            file_ext = os.path.splitext(input_file)[1].lower()
            if file_ext not in ['.json', '.csv']:
                print(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_ext}")
                print("ì§€ì› í˜•ì‹: .json, .csv")
                continue
            
            result = normalizer.process_file(input_file, save_to_db)
            
            print(f"\n=== ì²˜ë¦¬ ê²°ê³¼ ===")
            if result['status'] == 'SUCCESS':
                print(f"ìƒíƒœ: ì„±ê³µ")
                print(f"ì…ë ¥ íŒŒì¼: {result['input_file']}")
                print(f"ì…ë ¥ í˜•ì‹: {result['input_format'].upper()}")
                print(f"ì¶œë ¥ íŒŒì¼: {result['output_file']}")
                print(f"ì›ë³¸ í•­ëª© ìˆ˜: {result['original_count']}")
                print(f"ë³€í™˜ëœ í•­ëª© ìˆ˜: {result['normalized_count']}")
                print(f"ê°ì§€ëœ ì†ŒìŠ¤: {', '.join(result['detected_sources'])}")
            else:
                print(f"ìƒíƒœ: ì‹¤íŒ¨")
                print(f"ì˜¤ë¥˜: {result['error']}")

        elif choice == "3":
            stats = normalizer.get_database_stats()
            print(f"\n=== ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ===")
            print(f"ì´ ê²Œì‹œë¬¼ ìˆ˜ : {stats.get('total_posts', 0)}")
            print(f"ì´ IOC ìˆ˜ : {stats.get('total_iocs', 0)}")
            print(f"ì´ ê´€ê³„ ìˆ˜ : {stats.get('total_relationships', 0)}")
            print(f"ê²Œì‹œë¬¼ ì†ŒìŠ¤ë³„ ë¶„í¬: {stats.get('posts_by_source', {})}")
            print(f"ê²Œì‹œë¬¼ ìœ„í˜‘ ìœ í˜•ë³„ ë¶„í¬: {stats.get('posts_by_threat_type', {})}")
            print(f"IOC íƒ€ì…ë³„ ë¶„í¬: {stats.get('iocs_by_type', {})}")
            print(f"í™œì„± ì‘ì„±ì TOP 10: {stats.get('top_authors', {})}")
            print(f"ìµœê·¼ 7ì¼ê°„ ê²Œì‹œë¬¼ ìˆ˜: {stats.get('posts_last_7_days', 0)}") 
        
        elif choice == "4":
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()