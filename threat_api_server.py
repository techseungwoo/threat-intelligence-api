from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import json
import os
import tempfile
import asyncio
from datetime import datetime, timezone, timedelta
import logging
from pathlib import Path
import psycopg2
from urllib.parse import urlparse
# ê¸°ì¡´ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ import
from C_part_data_pipeline import MultiFormatThreatNormalizer, ThreatProcessingSystem

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="ìœ„í˜‘ì •ë³´ ë°ì´í„° ì •ì œ API",
    description="AíŒŒíŠ¸(ë‹¤í¬ì›¹), BíŒŒíŠ¸(í…”ë ˆê·¸ë¨) ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ DíŒŒíŠ¸(ëŒ€ì‹œë³´ë“œ)ìš© í†µí•© DB ì œê³µ",
    version="1.0.0"
)
def get_kst_time():
    """í•œêµ­ ì‹œê°„ ë°˜í™˜"""
    kst = timezone(timedelta(hours=9))
    return datetime.now(kst).strftime('%Y-%m-%d %H:%M:%S')

# CORS ì„¤ì • (ë‹¤ë¥¸ íŒŒíŠ¸ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ê¸€ë¡œë²Œ ë³€ìˆ˜ ì´ˆê¸°í™” - ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼í•œ DB ì‚¬ìš©
# ğŸ”¥ DB ì—°ê²° ë°©ì‹ ìë™ ê°ì§€
DATABASE_URL = os.getenv('DATABASE_URL')  # Railway PostgreSQL

if DATABASE_URL:
    # PostgreSQL ì‚¬ìš© (Railway í™˜ê²½)
    print("PostgreSQL ì‚¬ìš© (Railway)")
    DB_TYPE = "postgresql"
    DB_PATH = None  # PostgreSQLì—ì„œëŠ” ì‚¬ìš© ì•ˆí•¨
else:
    # SQLite ì‚¬ìš© (ë¡œì»¬ í™˜ê²½)
    print("SQLite ì‚¬ìš© (ë¡œì»¬)")
    DB_TYPE = "sqlite"
    DB_PATH = 'threat_intelligence.db'

# ğŸ”¥ DBë³„ ì´ˆê¸°í™”
if DB_TYPE == "postgresql":
    # PostgreSQLìš© - ì„ì‹œë¡œ None ì„¤ì • (ë‚˜ì¤‘ì— ìˆ˜ì •)
    normalizer = None
    threat_processor = None
else:
    # SQLiteìš© - ê¸°ì¡´ ë°©ì‹
    normalizer = MultiFormatThreatNormalizer(
        output_folder='api_processed_data',
        db_path=DB_PATH
    )
    threat_processor = ThreatProcessingSystem(DB_PATH)

def get_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•¨ìˆ˜"""
    try:
        if DB_TYPE == "postgresql":
            if not DATABASE_URL:
                raise Exception("DATABASE_URLì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return psycopg2.connect(DATABASE_URL)
        else:
            import sqlite3
            return sqlite3.connect(DB_PATH)
    except Exception as e:
        print(f"DB ì—°ê²° ì˜¤ë¥˜: {e}")
        raise e

def get_postgresql_stats():
    """PostgreSQLìš© í†µê³„ ì¡°íšŒ"""
    try:
        conn = get_db_connection()
        
        # PostgreSQLì€ withë¬¸ ì‚¬ìš©
        with conn.cursor() as cursor:
            stats = {}
            
            # ê¸°ë³¸ í†µê³„
            cursor.execute("SELECT COUNT(*) FROM threat_posts")
            result = cursor.fetchone()
            stats['total_posts'] = result[0] if result else 0
            
            # ì†ŒìŠ¤ë³„ ë¶„í¬
            cursor.execute("SELECT source_type, COUNT(*) FROM threat_posts GROUP BY source_type")
            stats['posts_by_source'] = dict(cursor.fetchall())
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            stats.update({
                'total_iocs': 0,
                'total_relationships': 0,
                'posts_by_threat_type': {},
                'iocs_by_type': {},
                'top_authors': {},
                'posts_last_7_days': 0
            })
        
        conn.close()
        return stats
        
    except Exception as e:
        print(f"PostgreSQL í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return {
            'total_posts': 0,
            'total_iocs': 0,
            'total_relationships': 0,
            'posts_by_source': {},
            'posts_by_threat_type': {},
            'iocs_by_type': {},
            'top_authors': {},
            'posts_last_7_days': 0
        }
def save_postgresql_data(normalized_data: List[Dict]) -> Dict:
    """PostgreSQLìš© ë°ì´í„° ì €ì¥ í•¨ìˆ˜"""
    try:
        conn = get_db_connection()
        saved_count = 0
        
        with conn.cursor() as cursor:
            for item in normalized_data:
                try:
                    # ì¤‘ë³µ ì²´í¬
                    cursor.execute("SELECT COUNT(*) FROM threat_posts WHERE id = %s", (item.get('thread_id', ''),))
                    if cursor.fetchone()[0] > 0:
                        continue
                    
                    # ë°ì´í„° ì‚½ì…
                    cursor.execute('''
                        INSERT INTO threat_posts 
                        (id, source_type, title, text, author, created_at)
                        VALUES (%s, %s, %s, %s, %s, CURRENT_TIMESTAMP)
                    ''', (
                        item.get('thread_id', f"auto-{saved_count}"),
                        item.get('source_type', ''),
                        item.get('title', ''),
                        item.get('text', ''),
                        item.get('author', '')
                    ))
                    saved_count += 1
                except Exception as e:
                    print(f"ê°œë³„ ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
                    continue
        
        conn.commit()
        conn.close()
        
        return {'saved': saved_count, 'duplicates': 0, 'errors': 0}
        
    except Exception as e:
        print(f"PostgreSQL ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
        return {'saved': 0, 'duplicates': 0, 'errors': 1}

def normalize_postgresql_item(item: Dict) -> Dict:
    """PostgreSQLìš© ë°ì´í„° ì •ê·œí™” í•¨ìˆ˜ (SQLiteì™€ ë™ì¼í•œ ë§¤í•‘ ì ìš©)"""
    
    # SQLiteì™€ ë™ì¼í•œ í•„ë“œ ë§¤í•‘ ì •ì˜
    field_mappings = {
        'thread_id': [
            'thread_id', 'Message ID', 'message_id', 'id', 'msg_id', 'post_id', 'event_id'
        ],
        'url': [
            'url', 'link', 'source_url', 'URL', 'Link'
        ],
        'keyword': [
            'keyword', 'Detected Keywords', 'detected_keywords', 
            'keywords', 'search_terms', 'query', 'Keywords'
        ],
        'found_at': [
            'found_at', 'Timestamp', 'timestamp', 'date_collected',
            'Found At', 'collected_at', 'crawled_at'
        ],
        'title': [
            'title', 'subject', 'headline', 'message_preview', 'Title','event_info'
        ],
        'text': [
            'text', 'Content', 'content', 'preview', 'message', 
            'description', 'hidden_content', 'Message', 'Text'
        ],
        'author': [
            'author', 'Channel', 'channel', 'username', 'user',
            'Author', 'User', 'Username', 'creator_org'
        ],
        'date': [
            'date', 'created_at', 'post_date', 'message_date',
            'Date', 'Created At', 'Post Date', 'event_date'
        ],
        'threat_type': [
            'threat_type', 'Threat Type', 'category', 'type',
            'Category', 'Type', 'threat_category'
        ],
        'platform': [
            'forum', 'platform', 'source_platform', 'site',
            'Forum', 'Platform', 'Source', 'sourcetype'
        ],
        'event_id': [
            'event_id', 'event_id', 'Event ID', 'id'
        ],
        'event_info': [
            'event_info', 'info', 'description'
        ],
        'event_date': [
            'event_date', 'event_timestamp'
        ]
    }
    
    def find_matching_field(data: Dict, target_field: str) -> str:
        """í‘œì¤€ í•„ë“œì— ë§¤í•‘ë˜ëŠ” ì›ë³¸ í•„ë“œëª… ì°¾ê¸°"""
        possible_names = field_mappings.get(target_field, [])
        
        for field_name in possible_names:
            if field_name in data:
                return field_name
            
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰
            for key in data.keys():
                if key.lower() == field_name.lower():
                    return key
        
        return None
    
    def detect_source_type(data: Dict) -> str:
        """ë°ì´í„° êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì†ŒìŠ¤ íƒ€ì… ìë™ ê°ì§€"""
        # MISP ê°ì§€ ë¡œì§
        if 'sourcetype' in data and data['sourcetype'] == 'MISP':
            return 'misp'
        if 'event_id' in data and 'creator_org' in data:
            return 'misp'
        if 'pii_data' in data and 'event_info' in data:
            return 'misp'
        
        # í…”ë ˆê·¸ë¨ ë°ì´í„° íŠ¹ì„± í™•ì¸
        telegram_indicators = [
            'Channel', 'Message ID', 'Threat Type', 'Detected Keywords',
            'channel', 'message_id', 'threat_type', 'detected_keywords'
        ]
        
        # ë‹¤í¬ì›¹ ë°ì´í„° íŠ¹ì„± í™•ì¸
        darkweb_indicators = [
            'forum', 'thread_id', 'has_hidden_content', 'preview',
            'Forum', 'Thread ID', 'author', 'url'
        ]
        
        telegram_score = sum(1 for indicator in telegram_indicators if indicator in data)
        darkweb_score = sum(1 for indicator in darkweb_indicators if indicator in data)
        
        if telegram_score > darkweb_score:
            return 'telegram'
        elif darkweb_score > telegram_score:
            return 'darkweb'
        else:
            # ê¸°ë³¸ê°’ ë˜ëŠ” ì¶”ê°€ íœ´ë¦¬ìŠ¤í‹± ì ìš©
            if any(indicator in data for indicator in ['Channel', 'Message ID', 'channel']):
                return 'telegram'
            elif any(indicator in data for indicator in ['forum', 'thread_id', 'Forum']):
                return 'darkweb'
            else:
                return 'unknown'
    
    def clean_text(text: Any) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ ë° ì •ê·œí™”"""
        if text is None:
            return ""
        
        import re
        text_str = str(text).strip()
        
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜
        text_str = re.sub(r'\s+', ' ', text_str)
        # HTML íƒœê·¸ ì œê±°
        text_str = re.sub(r'<[^>]+>', '', text_str)
        # ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
        text_str = text_str.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
        
        return text_str.strip()
    
    # ì‹œì‘: ì •ê·œí™” ì²˜ë¦¬
    normalized = {}
    
    # ì†ŒìŠ¤ íƒ€ì… ìë™ ê°ì§€
    source_type = detect_source_type(item)
    normalized['source_type'] = source_type
    
    # í‘œì¤€ í•„ë“œë“¤
    standard_fields = [
        'thread_id', 'url', 'keyword', 'found_at', 'title', 'text', 
        'author', 'date', 'threat_type', 'platform', 'event_id', 
        'event_info', 'event_date'
    ]
    
    # ê° í‘œì¤€ í•„ë“œì— ëŒ€í•´ ë§¤í•‘ ìˆ˜í–‰
    for standard_field in standard_fields:
        original_field = find_matching_field(item, standard_field)
        
        if original_field:
            value = item[original_field]
        else:
            value = ""
        
        # íŠ¹ë³„ ì²˜ë¦¬ê°€ í•„ìš”í•œ í•„ë“œë“¤
        if standard_field in ['text', 'title']:
            normalized[standard_field] = clean_text(value)
        elif standard_field == 'keyword':
            # ì—¬ëŸ¬ í‚¤ì›Œë“œê°€ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ê²½ìš° ì²˜ë¦¬
            if value:
                if isinstance(value, str):
                    keywords = value.split(',')
                    normalized[standard_field] = ', '.join([kw.strip() for kw in keywords])
                else:
                    normalized[standard_field] = str(value).strip()
            else:
                normalized[standard_field] = ""
        else:
            normalized[standard_field] = str(value).strip() if value else ""
    
    # ì†ŒìŠ¤ë³„ íŠ¹ë³„ ì²˜ë¦¬
    if source_type == 'telegram':
        # í…”ë ˆê·¸ë¨ì˜ ê²½ìš° ì±„ë„ëª…ì„ authorì™€ platform ë‘˜ ë‹¤ì— ì„¤ì •
        if normalized['author']:
            normalized['platform'] = normalized['author']
    elif source_type == 'darkweb':
        # ë‹¤í¬ì›¹ì˜ ê²½ìš° ìˆ¨ê²¨ì§„ ì½˜í…ì¸  ì •ë³´ ì¶”ê°€
        if 'has_hidden_content' in item and item['has_hidden_content']:
            hidden_content = item.get('hidden_content', '')
            if hidden_content:
                normalized['text'] += f" [Hidden Content: {clean_text(hidden_content)}]"
    elif source_type == 'misp':
        # PII ë°ì´í„°ë¥¼ textì— ì¶”ê°€
        if 'pii_data' in item and item['pii_data']:
            pii = item['pii_data']
            pii_parts = []
            
            if pii.get('name'):
                pii_parts.append(f"Name: {pii['name']}")
            if pii.get('username'):
                pii_parts.append(f"Username: {pii['username']}")
            if pii.get('email'):
                pii_parts.append(f"Email: {pii['email']}")
            if pii.get('password'):
                pii_parts.append("Password: [REDACTED]")
            if pii.get('phone'):
                pii_parts.append(f"Phone: {pii['phone']}")
            
            if pii_parts:
                current_text = normalized.get('text', '')
                pii_text = ' | '.join(pii_parts)
                normalized['text'] = f"{current_text} [PII: {pii_text}]".strip()
        
        # MISP ê¸°ë³¸ ì„¤ì •
        normalized['platform'] = 'MISP'
        normalized['threat_type'] = 'OSINT'
    
    # thread_idê°€ ì—†ëŠ” ê²½ìš° ìƒì„±
    if not normalized['thread_id']:
        import uuid
        normalized['thread_id'] = str(uuid.uuid4())[:8]
    
    return normalized

def search_postgresql_author(author: str, limit: int = 100) -> List[Dict]:
    """PostgreSQLìš© ì‘ì„±ì ê²€ìƒ‰"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if DB_TYPE == "postgresql":
            cursor.execute('''
                SELECT id, title, text, author, found_at, source_type, created_at
                FROM threat_posts 
                WHERE author ILIKE %s
                ORDER BY created_at DESC 
                LIMIT %s
            ''', (f'%{author}%', limit))
        else:
            cursor.execute('''
                SELECT id, title, text, author, found_at, source_type, created_at
                FROM threat_posts 
                WHERE author LIKE ?
                ORDER BY created_at DESC 
                LIMIT ?
            ''', (f'%{author}%', limit))
        
        posts = cursor.fetchall()
        conn.close()
        
        results = []
        for post in posts:
            results.append({
                "id": post[0],
                "title": post[1],
                "text": post[2][:200] + "..." if len(post[2]) > 200 else post[2],
                "author": post[3],
                "found_at": post[4],
                "source_type": post[5],
                "created_at": post[6]
            })
        
        return results
        
    except Exception as e:
        print(f"ì‘ì„±ì ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

async def export_as_sqlite():
    """
    PostgreSQL ë°ì´í„°ë¥¼ SQLite íŒŒì¼ë¡œ ë³€í™˜
    """
    try:
        import tempfile
        import sqlite3
        
        # ì„ì‹œ SQLite íŒŒì¼ ìƒì„±
        sqlite_file = f"/tmp/threat_db_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db"
        sqlite_conn = sqlite3.connect(sqlite_file)
        sqlite_cursor = sqlite_conn.cursor()
        
        # SQLite í…Œì´ë¸” ìƒì„± (ê¸°ì¡´ êµ¬ì¡°ì™€ ë™ì¼)
        sqlite_cursor.execute('''
            CREATE TABLE threat_posts (
                id TEXT PRIMARY KEY,
                source_type TEXT,
                thread_id TEXT,
                url TEXT,
                keyword TEXT,
                found_at TIMESTAMP,
                title TEXT,
                text TEXT,
                author TEXT,
                date TIMESTAMP,
                threat_type TEXT,
                platform TEXT,
                data_hash TEXT,
                created_at TIMESTAMP,
                event_id TEXT,
                event_info TEXT,
                event_date TIMESTAMP
            )
        ''')
        
        # PostgreSQLì—ì„œ ë°ì´í„° ì¡°íšŒ
        pg_conn = get_db_connection()
        pg_cursor = pg_conn.cursor()
        
        pg_cursor.execute('''
            SELECT id, 
                   COALESCE(source_type, 'unknown') as source_type,
                   COALESCE(id, '') as thread_id,
                   COALESCE(url, '') as url,
                   COALESCE(keyword, 'N/A') as keyword,
                   COALESCE(found_at, created_at) as found_at,
                   COALESCE(title, 'No Title') as title,
                   COALESCE(text, '') as text,
                   COALESCE(author, 'Unknown') as author,
                   COALESCE(date, created_at) as date,
                   COALESCE(threat_type, 'General') as threat_type,
                   COALESCE(platform, source_type) as platform,
                   COALESCE(id, '') as data_hash,
                   created_at,
                   COALESCE(event_id, '') as event_id,
                   COALESCE(event_info, '') as event_info,
                   COALESCE(event_date, created_at) as event_date
            FROM threat_posts
            ORDER BY created_at DESC
        ''')
        
        # SQLiteì— ë°ì´í„° ì‚½ì…
        rows = pg_cursor.fetchall()
        for row in rows:
            sqlite_cursor.execute('''
                INSERT INTO threat_posts VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)
            ''', row)
        
        pg_conn.close()
        sqlite_conn.commit()
        sqlite_conn.close()
        
        # SQLite íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì œê³µ
        return FileResponse(
            sqlite_file,
            media_type='application/octet-stream',
            filename=f"threat_db_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db"
        )
        
    except Exception as e:
        print(f"SQLite ë³€í™˜ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"SQLite ë³€í™˜ ì˜¤ë¥˜: {str(e)}")        

def init_postgresql_tables():
    """PostgreSQL í…Œì´ë¸” ì´ˆê¸°í™”í•œë‹¤"""
    if DB_TYPE != "postgresql":
        return
        
    try:
        conn = get_db_connection()
        
        with conn.cursor() as cursor:
            # threat_posts í…Œì´ë¸” ìƒì„±
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS threat_posts (
                    id TEXT PRIMARY KEY,
                    source_type TEXT,
                    thread_id TEXT,
                    url TEXT,
                    keyword TEXT,
                    found_at TIMESTAMP,
                    title TEXT,
                    text TEXT,
                    author TEXT,
                    date TIMESTAMP,
                    threat_type TEXT,
                    platform TEXT,
                    data_hash TEXT UNIQUE,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    event_id TEXT,
                    event_info TEXT,
                    event_date TIMESTAMP
                )
            ''')
        
        conn.commit()
        conn.close()
        print("PostgreSQL í…Œì´ë¸” ì´ˆê¸°í™” ì™„ë£Œ")
        
    except Exception as e:
        print(f"PostgreSQL ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

# PostgreSQL í™˜ê²½ì—ì„œ í…Œì´ë¸” ì´ˆê¸°í™”
if DB_TYPE == "postgresql":
    init_postgresql_tables()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# Pydantic ëª¨ë¸ë“¤ (API ìš”ì²­/ì‘ë‹µ êµ¬ì¡° ì •ì˜)
# =============================================================================

class ThreatDataItem(BaseModel):
    """ë‹¨ì¼ ìœ„í˜‘ì •ë³´ ë°ì´í„° í•­ëª©"""
    source_type: Optional[str] = None
    thread_id: Optional[str] = None
    url: Optional[str] = None
    keyword: Optional[str] = None
    found_at: Optional[str] = None
    title: Optional[str] = None
    text: Optional[str] = None
    author: Optional[str] = None
    date: Optional[str] = None
    threat_type: Optional[str] = None
    platform: Optional[str] = None
    event_id: Optional[str] = None
    event_info: Optional[str] = None
    event_date: Optional[str] = None
    pii_data: Optional[Dict[str, Any]] = None

class BulkThreatData(BaseModel):
    """ëŒ€ëŸ‰ ìœ„í˜‘ì •ë³´ ë°ì´í„°"""
    source: str  # 'darkweb' ë˜ëŠ” 'telegram'
    data: List[Dict[str, Any]]

class ProcessingResponse(BaseModel):
    """ì²˜ë¦¬ ê²°ê³¼ ì‘ë‹µ"""
    success: bool
    message: str
    processed_count: int
    new_posts: int
    related_posts: int
    duplicates: int
    errors: int
    batch_id: Optional[str] = None

class SearchRequest(BaseModel):
    """ê²€ìƒ‰ ìš”ì²­"""
    query: str
    search_type: str  # 'ioc', 'author', 'content'
    limit: Optional[int] = 100

# =============================================================================
# AíŒŒíŠ¸, BíŒŒíŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@app.post("/api/v1/data/upload/json", response_model=ProcessingResponse)
async def upload_json_data(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    source: str = "unknown"
):
    """
    AíŒŒíŠ¸, BíŒŒíŠ¸ì—ì„œ JSON íŒŒì¼ ì—…ë¡œë“œ
    """
    try:
        # íŒŒì¼ í˜•ì‹ ê²€ì¦
        if not file.filename.endswith('.json'):
            raise HTTPException(status_code=400, detail="JSON íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤")
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(mode='wb', suffix='.json', delete=False) as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
        background_tasks.add_task(process_uploaded_file, temp_file_path, source, file.filename)
        
        logger.info(f"JSON íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {file.filename} (ì†ŒìŠ¤: {source})")
        
        return ProcessingResponse(
            success=True,
            message=f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤.",
            processed_count=0,  # ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ì´ë¯€ë¡œ ì•„ì§ ëª¨ë¦„
            new_posts=0,
            related_posts=0,
            duplicates=0,
            errors=0
        )
        
    except Exception as e:
        logger.error(f"JSON ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

@app.post("/api/v1/data/upload/bulk", response_model=ProcessingResponse)
async def upload_bulk_data(data: BulkThreatData):
    """
    AíŒŒíŠ¸, BíŒŒíŠ¸ì—ì„œ JSON ë°ì´í„°ë¥¼ ì§ì ‘ POSTë¡œ ì „ì†¡
    """
    try:
        #ì†ŒìŠ¤ íƒ€ì… ê²€ì¦ 
        valid_sources = ['darkweb', 'telegram', 'misp']
        if data.source not in valid_sources:
            raise HTTPException(status_code=400, detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ íƒ€ì…: {data.source}")
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
        batch_size = 50
        total_items = len(data.data)

        logger.info(f"ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì‹ : {total_items}ê°œ í•­ëª© (ì†ŒìŠ¤: {data.source})")
        logger.info(f"ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œì”© ì²˜ë¦¬")
        
        all_stats = {'saved': 0, 'duplicates': 0, 'errors': 0}

        for i in range(0, total_items, batch_size):
            batch_data = data.data[i:i + batch_size]
            logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {i+1}-{min(i+batch_size, total_items)}/{total_items}")        
            
            # ë°ì´í„° ì •ì œ ë° í‘œì¤€í™”
            normalized_data = []
            for item in batch_data:
                if DB_TYPE == "postgresql":
                    normalized_item = normalize_postgresql_item(item)
                else:
                    normalized_item = normalizer.normalize_single_item(item)
                
                normalized_item['source_type'] = data.source
                normalized_data.append(normalized_item)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            if DB_TYPE == "postgresql":
                stats = save_postgresql_data(normalized_data)
            else:
                stats = normalizer.save_to_database(normalized_data)
                
            all_stats['saved'] += stats.get('saved', 0)
            all_stats['duplicates'] += stats.get('duplicates', 0)
            all_stats['errors'] += stats.get('errors', 0)

            if i + batch_size < total_items:
                await asyncio.sleep(2)
                logger.info(f"ë°°ì¹˜ íœ´ì‹ : 2ì´ˆ")
                
            logger.info(f"ë°°ì¹˜ {i // batch_size + 1} ì²˜ë¦¬ ì™„ë£Œ: {stats}")
        
        logger.info(f"ì „ì²´ ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {all_stats}")
        
        return ProcessingResponse(
            success=True,
            message="ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ",
            processed_count=total_items,
            new_posts=all_stats.get('saved', 0),
            related_posts=0,
            duplicates=all_stats.get('duplicates', 0),
            errors=all_stats.get('errors', 0)
        )
        
    except Exception as e:
        logger.error(f"ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

@app.post("/api/v1/data/upload/single")
async def upload_single_data(item: ThreatDataItem):
    """
    ë‹¨ì¼ ìœ„í˜‘ì •ë³´ ë°ì´í„° ì—…ë¡œë“œ (í…ŒìŠ¤íŠ¸ìš©)
    """
    try:
        # Pydantic ëª¨ë¸ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        item_dict = item.dict()
        
        # ì •ì œ ë° í‘œì¤€í™”
        if DB_TYPE == "postgresql":
            normalized_item = normalize_postgresql_item(item_dict)
            stats = save_postgresql_data([normalized_item])
        else:
            normalized_item = normalizer.normalize_single_item(item_dict)
            stats = normalizer.save_to_database([normalized_item])
        
        return {
            "success": True,
            "message": "ë‹¨ì¼ ë°ì´í„° ì €ì¥ ì™„ë£Œ",
            "stats": stats
        }
        
    except Exception as e:
        logger.error(f"ë‹¨ì¼ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

# =============================================================================
# DíŒŒíŠ¸ìš© ë°ì´í„° ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@app.get("/api/v1/data/stats")
async def get_database_stats():
    """
    DíŒŒíŠ¸ìš© ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ í†µê³„ ì œê³µ
    """
    try:
        if DB_TYPE == "postgresql":
            stats = get_postgresql_stats()
        else:
            stats = normalizer.get_database_stats()
        return {
            "success": True,
            "data": stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")

@app.post("/api/v1/data/search")
async def search_threat_data(request: SearchRequest):
    """
    DíŒŒíŠ¸ìš© ìœ„í˜‘ì •ë³´ ê²€ìƒ‰ API
    """
    try:
        # ğŸ”¥ ëª¨ë“  ê²€ìƒ‰ì„ PostgreSQL ê¸°ë°˜ìœ¼ë¡œ í†µì¼
        if request.search_type == "author":
            results = search_postgresql_author(request.query, request.limit)
        elif request.search_type == "ioc":
            # IOC ê²€ìƒ‰ë„ ê¸°ë³¸ í…ìŠ¤íŠ¸ ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬
            results = search_content(request.query, request.limit)
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰
            results = search_content(request.query, request.limit)
        
        return {
            "success": True,
            "query": request.query,
            "search_type": request.search_type,
            "result_count": len(results),
            "data": results
        }
        
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")

@app.get("/api/v1/data/recent")
async def get_recent_threats(limit: int = 100, source_type: str = None):
    """
    DíŒŒíŠ¸ìš© ìµœì‹  ìœ„í˜‘ì •ë³´ ì¡°íšŒ
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if DB_TYPE == "postgresql":
            # PostgreSQLìš© ì¿¼ë¦¬ (%s ì‚¬ìš©)
            if source_type:
                cursor.execute('''
                    SELECT id, title, text, author, found_at, source_type, threat_type, 
                            event_id, event_info
                    FROM threat_posts 
                    WHERE source_type = %s
                    ORDER BY created_at DESC 
                    LIMIT %s
                ''', (source_type, limit))
            else:
                cursor.execute('''
                    SELECT id, title, text, author, found_at, source_type, threat_type,
                            event_id, event_info
                    FROM threat_posts 
                    ORDER BY created_at DESC 
                    LIMIT %s
                ''', (limit,))
        else:
            # SQLiteìš© ì¿¼ë¦¬ (? ì‚¬ìš©)
            if source_type:
                cursor.execute('''
                    SELECT id, title, text, author, found_at, source_type, threat_type, 
                            event_id, event_info
                    FROM threat_posts 
                    WHERE source_type = ?
                    ORDER BY created_at DESC 
                    LIMIT ?
                ''', (source_type, limit))
            else:
                cursor.execute('''
                    SELECT id, title, text, author, found_at, source_type, threat_type,
                            event_id, event_info
                    FROM threat_posts 
                    ORDER BY created_at DESC 
                    LIMIT ?
                ''', (limit,))
        
        posts = cursor.fetchall()
        conn.close()
        
        results = []
        for post in posts:
            result_item = {  
                "id": post[0],
                "title": post[1],
                "text": post[2][:200] + "..." if len(post[2]) > 200 else post[2],
                "author": post[3],
                "found_at": post[4],
                "source_type": post[5],
                "threat_type": post[6]
            }  
    
            # ğŸ†• MISP ë°ì´í„°ì¸ ê²½ìš° ì¶”ê°€ ì •ë³´ í¬í•¨
            if post[5] == 'misp':
                result_item.update({
                    "event_id": post[7],
                    "event_info": post[8]
                })
    
            results.append(result_item)
        
        return {
            "success": True,
            "data": results,
            "count": len(results)
        }
        
    except Exception as e:
        logger.error(f"ìµœì‹  ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")

@app.get("/api/v1/data/export/db")
async def export_database():
    """
    DíŒŒíŠ¸ìš© ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    """
    try:
        if DB_TYPE == "postgresql":
            # PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ë¥¼ SQLiteë¡œ ë³€í™˜í•˜ì—¬ ë‹¤ìš´ë¡œë“œ
            return await export_as_sqlite()
        
        if os.path.exists(DB_PATH):
            return FileResponse(
                DB_PATH,
                media_type='application/octet-stream',
                filename=f"threat_db_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db"
            )
        else:
            raise HTTPException(status_code=404, detail="ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
    
    except Exception as e:
        logger.error(f"DB ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜: {str(e)}")

# =============================================================================
# í—¬í¼ í•¨ìˆ˜ë“¤
# =============================================================================

async def process_uploaded_file(file_path: str, source: str, original_filename: str):
    """
    ì—…ë¡œë“œëœ íŒŒì¼ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
    """
    try:
        logger.info(f"ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‹œì‘: {original_filename}")
        
        if DB_TYPE == "postgresql":
            # ğŸ”¥ PostgreSQLìš© íŒŒì¼ ì²˜ë¦¬ ì¶”ê°€
            import json
            
            # JSON íŒŒì¼ ì½ê¸°
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if isinstance(data, dict):
                data = [data]
            elif not isinstance(data, list):
                logger.error(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” JSON í˜•ì‹: {type(data)}")
                return
            
            # ë°ì´í„° ì •ê·œí™” ë° ì €ì¥
            normalized_data = []
            for item in data:
                normalized_item = normalize_postgresql_item(item)
                normalized_item['source_type'] = source
                normalized_data.append(normalized_item)
            
            # PostgreSQLì— ì €ì¥
            stats = save_postgresql_data(normalized_data)
            logger.info(f"PostgreSQL ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì™„ë£Œ: {original_filename} - {stats['saved']}ê°œ ì €ì¥")
            
        else:
            # SQLiteì—ì„œë§Œ ê¸°ì¡´ íŒŒì¼ ì²˜ë¦¬
            result = normalizer.process_file(file_path, save_to_db=True)
            
            if result['status'] == 'SUCCESS':
                logger.info(f"ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì™„ë£Œ: {original_filename} - {result['normalized_count']}ê°œ í•­ëª©")
            else:
                logger.error(f"ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {original_filename} - {result.get('error', 'Unknown error')}")
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(file_path)
            
    except Exception as e:
        logger.error(f"ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì˜¤ë¥˜ {original_filename}: {e}")
        # ì˜¤ë¥˜ ë°œìƒí•´ë„ ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            os.unlink(file_path)
        except:
            pass

def search_content(query: str, limit: int = 100) -> List[Dict]:
    """
    ì œëª©, ë‚´ìš©ì—ì„œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (PostgreSQL/SQLite í˜¸í™˜)
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if DB_TYPE == "postgresql":
            cursor.execute('''
                SELECT id, title, text, author, found_at, source_type
                FROM threat_posts 
                WHERE title ILIKE %s OR text ILIKE %s
                ORDER BY created_at DESC 
                LIMIT %s
            ''', (f'%{query}%', f'%{query}%', limit))
        else:
            cursor.execute('''
                SELECT id, title, text, author, found_at, source_type
                FROM threat_posts 
                WHERE title LIKE ? OR text LIKE ?
                ORDER BY created_at DESC 
                LIMIT ?
            ''', (f'%{query}%', f'%{query}%', limit))
        
        posts = cursor.fetchall()
        conn.close()
        
        results = []
        for post in posts:
            results.append({
                "id": post[0],
                "title": post[1],
                "text": post[2][:300] + "..." if len(post[2]) > 300 else post[2],
                "author": post[3],
                "found_at": post[4],
                "source_type": post[5]
            })
        
        return results
        
    except Exception as e:
        print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# =============================================================================
# ì„œë²„ ì‹¤í–‰ ë° ìƒíƒœ í™•ì¸
# =============================================================================

@app.get("/")
async def root():
    """
    API ì„œë²„ ìƒíƒœ í™•ì¸
    """
    return {
        "service": "ìœ„í˜‘ì •ë³´ ë°ì´í„° ì •ì œ API",
        "version": "1.0.0",
        "status": "running",
        "timestamp": get_kst_time(),
        "endpoints": {
            "AíŒŒíŠ¸_BíŒŒíŠ¸ìš©": {
                "JSONíŒŒì¼ì—…ë¡œë“œ": "/api/v1/data/upload/json",
                "ëŒ€ëŸ‰ë°ì´í„°ì „ì†¡": "/api/v1/data/upload/bulk",
                "ë‹¨ì¼ë°ì´í„°ì „ì†¡": "/api/v1/data/upload/single"
            },
            "DíŒŒíŠ¸ìš©": {
                "í†µê³„ì¡°íšŒ": "/api/v1/data/stats",
                "ë°ì´í„°ê²€ìƒ‰": "/api/v1/data/search",
                "ìµœì‹ ë°ì´í„°": "/api/v1/data/recent",
                "DBë‹¤ìš´ë¡œë“œ": "/api/v1/data/export/db"
            }
        }
    }

@app.get("/health")
async def health_check():
    """
    í—¬ìŠ¤ ì²´í¬
    """
    try:
        if DB_TYPE == "postgresql":
            stats = get_postgresql_stats()
        else:
            stats = normalizer.get_database_stats()
            
        return {
            "status": "healthy",
            "database": "connected",
            "total_posts": stats.get('total_posts', 0),
            "timestamp": get_kst_time()
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": get_kst_time()
        }
@app.post("/api/v1/admin/fix-timezone")
async def fix_timezone():
    """ê¸°ì¡´ ë°ì´í„°ì˜ ì‹œê°„ëŒ€ë¥¼ KSTë¡œ ìˆ˜ì • (1íšŒì„±)"""
    try:
        if DB_TYPE == "postgresql":
            raise HTTPException(
                status_code=400, 
                detail="PostgreSQL í™˜ê²½ì—ì„œëŠ” ì‹œê°„ëŒ€ ìˆ˜ì •ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì§ì ‘ ìˆ˜ì •í•˜ì„¸ìš”."
            )
        import sqlite3
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # ë¨¼ì € í˜„ì¬ ë°ì´í„° í™•ì¸
        cursor.execute("SELECT id, created_at FROM threat_posts LIMIT 3")
        sample_data = cursor.fetchall()
        logger.info(f"ìƒ˜í”Œ ë°ì´í„°: {sample_data}")
        
        # ë” ê°•ë ¥í•œ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ (í˜•ì‹ ìƒê´€ì—†ì´)
        update_queries = [
            # threat_posts í…Œì´ë¸”
            """UPDATE threat_posts 
               SET created_at = datetime(julianday(created_at) + 0.375)
               WHERE created_at IS NOT NULL""",
            
            """UPDATE threat_posts 
               SET date = datetime(julianday(date) + 0.375)
               WHERE date IS NOT NULL AND date != ''""",
            
            """UPDATE threat_posts 
               SET found_at = datetime(julianday(found_at) + 0.375)
               WHERE found_at IS NOT NULL AND found_at != ''""",
            
            # threat_iocs í…Œì´ë¸”
            """UPDATE threat_iocs 
               SET first_seen = datetime(julianday(first_seen) + 0.375)
               WHERE first_seen IS NOT NULL""",
            
            # post_relationships í…Œì´ë¸”
            """UPDATE post_relationships 
               SET created_at = datetime(julianday(created_at) + 0.375)
               WHERE created_at IS NOT NULL""",
            
            # processing_statistics í…Œì´ë¸”
            """UPDATE processing_statistics 
               SET processed_at = datetime(julianday(processed_at) + 0.375)
               WHERE processed_at IS NOT NULL"""
        ]
        
        total_affected = 0
        for i, query in enumerate(update_queries):
            cursor.execute(query)
            affected = cursor.rowcount
            total_affected += affected
            logger.info(f"ì¿¼ë¦¬ {i+1}: {affected}í–‰ ìˆ˜ì •")
        
        conn.commit()
        
        # ìˆ˜ì • í›„ ë°ì´í„° í™•ì¸
        cursor.execute("SELECT id, created_at FROM threat_posts LIMIT 3")
        updated_data = cursor.fetchall()
        logger.info(f"ìˆ˜ì • í›„ ë°ì´í„°: {updated_data}")
        
        conn.close()
        
        return {
            "success": True, 
            "message": f"ì´ {total_affected}ê°œ ë ˆì½”ë“œì˜ ì‹œê°„ì„ KSTë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤",
            "affected_rows": total_affected,
            "sample_before": sample_data,
            "sample_after": updated_data
        }
        
    except Exception as e:
        logger.error(f"ì‹œê°„ëŒ€ ìˆ˜ì • ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì‹œê°„ëŒ€ ìˆ˜ì • ì˜¤ë¥˜: {str(e)}")

@app.post("/api/v1/test/create-dummy")
async def create_dummy_data():
    """ì˜êµ¬ ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸ìš©"""
    try:
        conn = get_db_connection()  # ğŸ”¥ ë³€ê²½
        cursor = conn.cursor()
        
        if DB_TYPE == "postgresql":
            # PostgreSQLìš© ì¿¼ë¦¬
            cursor.execute('''
                INSERT INTO threat_posts 
                (id, source_type, title, text, author, created_at)
                VALUES (%s, %s, %s, %s, %s, CURRENT_TIMESTAMP)
            ''', ('test-persist-001', 'test', 'Persistence Test', 'This data should survive redeployment', 'test_user'))
        else:
            # SQLiteìš© ì¿¼ë¦¬
            cursor.execute('''
                INSERT INTO threat_posts 
                (id, source_type, title, text, author, created_at)
                VALUES (?, ?, ?, ?, ?, datetime('now', 'localtime'))
            ''', ('test-persist-001', 'test', 'Persistence Test', 'This data should survive redeployment', 'test_user'))
        
        conn.commit()
        conn.close()
        
        return {"success": True, "message": "í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ", "db_type": DB_TYPE}
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.get("/api/v1/debug/db-status")
async def debug_db_status():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸"""
    try:
        return {
            "DB_TYPE": DB_TYPE,
            "DATABASE_URL_exists": bool(DATABASE_URL),
            "DATABASE_URL_start": DATABASE_URL[:20] + "..." if DATABASE_URL else None,
            "psycopg2_imported": "psycopg2" in globals(),
        }
    except Exception as e:
        return {"error": str(e)}
# =============================================================================
# ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
# =============================================================================

if __name__ == "__main__":
    import uvicorn
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì½”ë“œ
    if DB_TYPE == "postgresql":
        init_postgresql_tables()
    else:
        threat_processor.init_advanced_database()
    
    print("=== ìœ„í˜‘ì •ë³´ ë°ì´í„° ì •ì œ API ì„œë²„ ===")
    print("AíŒŒíŠ¸(ë‹¤í¬ì›¹), BíŒŒíŠ¸(í…”ë ˆê·¸ë¨) â†’ CíŒŒíŠ¸(ì •ì œ) â†’ DíŒŒíŠ¸(ëŒ€ì‹œë³´ë“œ)")
    print()
    print("ì„œë²„ ì£¼ì†Œ: http://localhost:8000")
    print("API ë¬¸ì„œ: http://localhost:8000/docs")
    print("í—¬ìŠ¤ ì²´í¬: http://localhost:8000/health")
    print()
    
    # ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        "threat_api_server:app",  # ì´ íŒŒì¼ëª…ì´ threat_api_server.pyë¼ê³  ê°€ì •
        host="0.0.0.0",
        port=8000,
        reload=True,  # ê°œë°œ ì¤‘ì—ëŠ” True, í”„ë¡œë•ì…˜ì—ì„œëŠ” False
        log_level="info"
    )